<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Improving interpretability of polygenic scores using only summary statistics</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">GenoPred</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/opain/GenoPred">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Improving interpretability of polygenic scores using only summary statistics</h1>

</div>


<style>
p.caption {
  font-size: 1.5em;
}
</style>
<style type="text/css">
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
</style>
<hr />
<p>This page describes a project investigatin approaches for converting polygenic scores into interpretable information.</p>
<p>Aims:</p>
<ul>
<li>Develop method for converting polygenic Z-scores into absolute estimates using summary statistics</li>
<li>Develop figures to represent absolute risk</li>
<li>Validate approach for estimating PRS R2/AUC from summary statistics</li>
</ul>
<hr />
<div id="converting-polygenic-z-scores-to-absolute-estimates" class="section level1">
<h1><span class="header-section-number">1</span> Converting polygenic Z-scores to absolute estimates</h1>
<p>To enable correct intepretation of a polygenic score, the variance explained by the polygenic score must be considered. Furthermore, for binary outcomes the population prevelance must be considered, and for continuous outcomes the population mean and SD must be considered. It is possible to convert relative genetic risk into absolute esimates of an outcome when observed data is available, as 23andMe do, by splitting participants into genetic risk quantiles, and then estimating the mean outcome within each quantile. However, observed data is often not available. Here, we use an alternative approach based on summary statistics only alone.</p>
<hr />
<div id="describe-conversion" class="section level2">
<h2><span class="header-section-number">1.1</span> Describe conversion</h2>
<hr />
<div id="binary-outcomes" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Binary outcomes</h3>
<p>To convert a polygenic Z-score into an absolute estimate of risk, we must know the predicitve utility of the polygenic score (AUC), and the prevelance of the outcome in the general population. Then it is possible to estimate the proportion of cases within each polygenic score quantile using bivariate-normal distribution.</p>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code># Thank you for Alex Gillet for her work developing this code.
ccprobs.f &lt;- function(PRS_auc=0.641, prev=0.7463, n_quantile=20){
  
    # Convert AUC into cohen&#39;s d
    d &lt;- sqrt(2)*qnorm(PRS_auc)
    
    # Set mean difference between cases and control polygenic scores
    mu_case &lt;- d
    mu_control &lt;- 0
    
    # Estimate mean and variance of polygenic scores across case and control
    varPRS &lt;- prev*(1+(d^2) - (d*prev)^2) + (1-prev)*(1 - (d*prev)^2)
    E_PRS &lt;- d*prev
    
    # Estimate polygenic score quantiles
    by_quant&lt;-1/n_quantile
    p_quant &lt;- seq(by_quant, 1-by_quant, by=by_quant)
    quant_vals_PRS &lt;- rep(0, length(p_quant))
    quant_f_solve &lt;- function(x, prev, d, pq){prev*pnorm(x-d) + (1-prev)*pnorm(x) - pq}
    for(i in 1:length(p_quant)){
        quant_vals_PRS[i] &lt;- unlist(uniroot(quant_f_solve, prev=prev, d=d, pq= p_quant[i], interval=c(-2.5, 2.5), extendInt = &quot;yes&quot;, tol=6e-12)$root)
    }
    
    # Create a table for output
    ul_qv_PRS &lt;- matrix(0, ncol=2, nrow=n_quantile)
    ul_qv_PRS[1,1] &lt;- -Inf
    ul_qv_PRS[2:n_quantile,1] &lt;- quant_vals_PRS
    ul_qv_PRS[1:(n_quantile-1),2] &lt;- quant_vals_PRS
    ul_qv_PRS[n_quantile,2] &lt;- Inf
    
    ul_qv_PRS&lt;-cbind(ul_qv_PRS, (ul_qv_PRS[,1:2]-E_PRS)/sqrt(varPRS))
    
    # Estimate case control proportion for each quantile
    prob_quantile_case &lt;- pnorm(ul_qv_PRS[,2], mean = mu_case) - pnorm(ul_qv_PRS[,1], mean = mu_case)
    prob_quantile_control &lt;- pnorm(ul_qv_PRS[,2], mean = mu_control) - pnorm(ul_qv_PRS[,1], mean = mu_control)
    p_case_quantile &lt;- (prob_quantile_case*prev)/by_quant
    p_cont_quantile &lt;- (prob_quantile_control*(1-prev))/by_quant
    
    # Estimate OR comparing each quantile to bottom quantile
    OR &lt;- p_case_quantile/p_cont_quantile
    OR &lt;- OR/OR[1]
    
    # Return output
    out &lt;- cbind(ul_qv_PRS[,3:4],p_cont_quantile, p_case_quantile, OR)
    row.names(out) &lt;- 1:n_quantile
    colnames(out) &lt;- c(&quot;q_min&quot;, &quot;q_max&quot;,&quot;p_control&quot;, &quot;p_case&quot;, &quot;OR&quot;)
    
    data.frame(out)
}</code></pre>
</details>
<hr />
<div id="validate-conversion" class="section level4">
<h4><span class="header-section-number">1.1.1.1</span> Validate conversion</h4>
<p>Calculate reference-standardised polygenic scores within UK Biobank for a range of dichotomous phenotypes. Estimate the AUC/R2 of the polygenic scores in UKB. Compare measured and estimated absolute risk per PRS quantile. Use the PRScs fully baysian (pseudovalidation) polygenic scores, as this method provides a single score with good relative performance compared to other approaches.</p>
<p>Reference-standardised polygenic scores have already been calculated in UKB for the PRS methods comparison study, and the AUC has already been estimated. Read in polygenic scores and observed phenotype for UKB, measure proportion of cases per PRS quantile, and then estimate proportion of cases per PRS quantile.</p>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Target_scoring.config&#39;)

pheno=c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
gwas=c(&#39;DEPR06&#39;,&#39;DIAB05&#39;,&#39;COAD01&#39;,&#39;CROH01&#39;,&#39;SCLE03&#39;,&#39;RHEU02&#39;,&#39;BRCA01&#39;,&#39;PRCA01&#39;)

n_quant&lt;-20

files&lt;-data.frame(pheno,gwas)

# Create function
ccprobs.f &lt;- function(PRS_auc=0.641, prev=0.7463, n_quantile=20){
  
    # Convert AUC into cohen&#39;s d
    d &lt;- sqrt(2)*qnorm(PRS_auc)
    
    # Set mean difference between cases and control polygenic scores
    mu_case &lt;- d
    mu_control &lt;- 0
    
    # Estimate mean and variance of polygenic scores across case and control
    varPRS &lt;- prev*(1+(d^2) - (d*prev)^2) + (1-prev)*(1 - (d*prev)^2)
    E_PRS &lt;- d*prev
    
    # Estimate polygenic score quantiles
    by_quant&lt;-1/n_quantile
    p_quant &lt;- seq(by_quant, 1-by_quant, by=by_quant)
    quant_vals_PRS &lt;- rep(0, length(p_quant))
    quant_f_solve &lt;- function(x, prev, d, pq){prev*pnorm(x-d) + (1-prev)*pnorm(x) - pq}
    for(i in 1:length(p_quant)){
        quant_vals_PRS[i] &lt;- unlist(uniroot(quant_f_solve, prev=prev, d=d, pq= p_quant[i], interval=c(-2.5, 2.5), extendInt = &quot;yes&quot;, tol=6e-12)$root)
    }
    
    # Create a table for output
    ul_qv_PRS &lt;- matrix(0, ncol=2, nrow=n_quantile)
    ul_qv_PRS[1,1] &lt;- -Inf
    ul_qv_PRS[2:n_quantile,1] &lt;- quant_vals_PRS
    ul_qv_PRS[1:(n_quantile-1),2] &lt;- quant_vals_PRS
    ul_qv_PRS[n_quantile,2] &lt;- Inf
    
    ul_qv_PRS&lt;-cbind(ul_qv_PRS, (ul_qv_PRS[,1:2]-E_PRS)/sqrt(varPRS))
    
    # Estimate case control proportion for each quantile
    prob_quantile_case &lt;- pnorm(ul_qv_PRS[,2], mean = mu_case) - pnorm(ul_qv_PRS[,1], mean = mu_case)
    prob_quantile_control &lt;- pnorm(ul_qv_PRS[,2], mean = mu_control) - pnorm(ul_qv_PRS[,1], mean = mu_control)
    p_case_quantile &lt;- (prob_quantile_case*prev)/by_quant
    p_cont_quantile &lt;- (prob_quantile_control*(1-prev))/by_quant
    
    # Estimate OR comparing each quantile to bottom quantile
    OR &lt;- p_case_quantile/p_cont_quantile
    OR &lt;- OR/OR[1]
    
    # Return output
    out &lt;- cbind(ul_qv_PRS[,3:4],p_cont_quantile, p_case_quantile, OR)
    row.names(out) &lt;- 1:n_quantile
    colnames(out) &lt;- c(&quot;q_min&quot;, &quot;q_max&quot;,&quot;p_control&quot;, &quot;p_case&quot;, &quot;OR&quot;)
    
    data.frame(out)
}

# Run analysis for each phenotype
res_all&lt;-NULL
cor_res&lt;-NULL
plots_all&lt;-list()

for(i in 1:dim(files)[1]){
  # Read in pheno and prs data, and merge
  pheno_i&lt;-fread(paste0(UKBB_output,&#39;/Phenotype/PRS_comp_subset/UKBB.&#39;,files$pheno[i],&#39;.txt&#39;))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  prs_i&lt;-fread(paste0(UKBB_output,&#39;/PRS_for_comparison/1KG_ref/PRScs/&#39;,files$gwas[i],&#39;/UKBB.subset.w_hm3.&#39;,files$gwas[i],&#39;.PRScs_profiles&#39;))
  prs_i&lt;-prs_i[,c(&#39;FID&#39;,&#39;IID&#39;,paste0(files$gwas[i], &#39;_phiauto&#39;)), with=F]
  names(prs_i)[3]&lt;-&#39;prs&#39;

  pheno_prs&lt;-merge(pheno_i, prs_i, by=c(&#39;FID&#39;,&#39;IID&#39;))
  
  # Read in AUC for PRS
  assoc&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,files$pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,files$gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))
  prs_auc&lt;-assoc[grepl(&#39;phiauto&#39;, assoc$Predictor),]$AUC
  
  # Assign individuals to observed PRS quantiles
  obs_quant&lt;-quantile(pheno_prs$prs, prob = seq(0, 1, length = n_quant+1))
  pheno_prs$obs_quant&lt;-as.numeric(cut( pheno_prs$prs, obs_quant, include.lowest = T))

  # Calculate proportion of each quantile that are cases
  obs_cc&lt;-NULL
  for(k in 1:n_quant){
    obs_cc&lt;-rbind(obs_cc, data.frame(Phenotype=files$pheno[i],
                                     Type=&#39;Observed&#39;,
                                     Quantile=k,
                                     q_min=obs_quant[k],
                                     q_max=obs_quant[k+1],
                                     p_control=1-mean(pheno_prs$pheno[pheno_prs$obs_quant == k]),
                                     p_case=mean(pheno_prs$pheno[pheno_prs$obs_quant == k])))
  }
  
  # Assign individuals to estimated PRS quantiles
  est_cc&lt;-ccprobs.f(PRS_auc = prs_auc, prev=mean(pheno_prs$pheno), n_quantile = n_quant)
  est_cc$OR&lt;-NULL
  est_cc&lt;-data.frame(Phenotype=files$pheno[i],Type=&#39;Estimated&#39;,Quantile=1:n_quant, est_cc)
  est_quant&lt;-sort(unique(c(est_cc$q_min, est_cc$q_max)))
  pheno_prs$est_quant&lt;-as.numeric(cut( pheno_prs$prs, est_quant, include.lowest = T))
  
  tmp&lt;-cor.test(obs_cc$p_case, est_cc$p_case)
  
  # Estimate correlation between observed and expected
  cor_res&lt;-rbind(cor_res,data.frame(Phenotype=files$pheno[i],
                                    Cor=tmp$estimate,
                                    Low95CI=tmp$conf.int[1],
                                    High95CI=tmp$conf.int[2],
                                    N=length(pheno_prs$pheno),
                                    Ncas=sum(pheno_prs$pheno == 1),
                                    Ncon=sum(pheno_prs$pheno == 0)))

  quant_comp&lt;-rbind(obs_cc, est_cc)
  
  res_all&lt;-rbind(res_all, quant_comp)
  
  library(ggplot2)
  library(cowplot)
  
  plots_all[[i]]&lt;-ggplot(quant_comp, aes(x=Quantile, y=p_case, colour=Type)) +
                    geom_point(alpha=0.8) +
                    geom_line(alpha=0.8) +
                    labs(x=&#39;PRS quantile&#39;, y=paste0(&#39;Proportion with &#39;,files$pheno[i]), title=files$pheno[i], colour=&#39;Method&#39;) +
                    theme_cowplot(12)
}

png(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/PropCC_Comp.png&#39;), units=&#39;px&#39;, res=300, width=2000, height=3000)
  plot_grid(plotlist=plots_all, ncol = 2)
dev.off()

write.csv(cor_res, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/PropCC_Comp.csv&#39;, row.names=F, quote=F)</code></pre>
</details>
<details>
<p><summary>Show results</summary> <img src="Images/Validating_ImputeMe_Ideas/PropCC_Comp.png" alt="Observed-Estimated Proportion of Cases within Polygenic Score Quantiles" /></p>
<table>
<caption>Correlation between observed and estimated proportion of cases within polygenic score quantiles</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">Cor</th>
<th align="right">Low95CI</th>
<th align="right">High95CI</th>
<th align="right">N</th>
<th align="right">Ncas</th>
<th align="right">Ncon</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Depression</td>
<td align="right">0.9937111</td>
<td align="right">0.9838077</td>
<td align="right">0.9975649</td>
<td align="right">49999</td>
<td align="right">24999</td>
<td align="right">25000</td>
</tr>
<tr class="even">
<td align="left">T2D</td>
<td align="right">0.9954595</td>
<td align="right">0.9882933</td>
<td align="right">0.9982428</td>
<td align="right">49999</td>
<td align="right">14888</td>
<td align="right">35111</td>
</tr>
<tr class="odd">
<td align="left">CAD</td>
<td align="right">0.9958911</td>
<td align="right">0.9894026</td>
<td align="right">0.9984101</td>
<td align="right">49999</td>
<td align="right">25000</td>
<td align="right">24999</td>
</tr>
<tr class="even">
<td align="left">IBD</td>
<td align="right">0.9809581</td>
<td align="right">0.9514614</td>
<td align="right">0.9925978</td>
<td align="right">49999</td>
<td align="right">3461</td>
<td align="right">46538</td>
</tr>
<tr class="odd">
<td align="left">MultiScler</td>
<td align="right">0.9733777</td>
<td align="right">0.9325383</td>
<td align="right">0.9896268</td>
<td align="right">49999</td>
<td align="right">1137</td>
<td align="right">48862</td>
</tr>
<tr class="even">
<td align="left">RheuArth</td>
<td align="right">0.9844922</td>
<td align="right">0.9603604</td>
<td align="right">0.9939782</td>
<td align="right">49999</td>
<td align="right">3408</td>
<td align="right">46591</td>
</tr>
<tr class="odd">
<td align="left">Breast_Cancer</td>
<td align="right">0.9954801</td>
<td align="right">0.9883461</td>
<td align="right">0.9982508</td>
<td align="right">49999</td>
<td align="right">8512</td>
<td align="right">41487</td>
</tr>
<tr class="even">
<td align="left">Prostate_Cancer</td>
<td align="right">0.9969747</td>
<td align="right">0.9921906</td>
<td align="right">0.9988298</td>
<td align="right">50000</td>
<td align="right">2927</td>
<td align="right">47073</td>
</tr>
</tbody>
</table>
<p>Median Cor. = 0.9945853; Mean Cor. = 0.9895431; Min. Cor. = 0.9733777; Max. Cor. = 0.9969747</p>
</details>
<hr />
</div>
</div>
<div id="continuous-outcomes" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Continuous outcomes</h3>
<p>To convert a polygenic Z-score into an absolute estimate for a trait, we must know the predicitve utility of the polygenic score (R2), and the mean and SD of the outcome in the general population. Then it is possible to estimate the mean and SD of the trait within each polygenic score quantile using a truncated norm model (currently not theory based).</p>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code># Thank you for Alex Gillet for her work developing this code.
mean_sd_quant.f &lt;- function(PRS_R2=0.641, Outcome_mean=1, Outcome_sd=1, n_quantile=20){
  ### PRS quantiles with a continuous phenotype (Y)
  library(tmvtnorm)
  ###
  E_PRS = 0
  SD_PRS = sqrt(1)
  E_phenotype = Outcome_mean
  SD_phenotype = Outcome_sd 

  by_quant&lt;-1/(n_quantile)
  PRS_quantile_bounds &lt;- qnorm(p=seq(0, 1, by=by_quant), mean= E_PRS, sd= SD_PRS)
  lower_PRS_vec &lt;- PRS_quantile_bounds[1:n_quantile]
  upper_PRS_vec &lt;- PRS_quantile_bounds[2:(n_quantile+1)]
  
  mean_vec &lt;- c(E_phenotype, E_PRS)
  sigma_mat &lt;- matrix(sqrt(PRS_R2)*SD_phenotype*SD_PRS, nrow=2, ncol=2)
  sigma_mat[1,1] &lt;- SD_phenotype^2
  sigma_mat[2,2] &lt;- SD_PRS^2
  
  ### mean of phenotype within the truncated PRS distribution
  out_mean_Y &lt;- rep(0, 20)
  ### SD of phenotype within the truncated PRS distribution
  out_SD_Y &lt;- rep(0, 20)
  ### cov of Y and PRS given truncation on PRS
  out_cov_Y_PRS &lt;- rep(0, 20)
  ### SD of PRS given truncation on PRS
  out_SD_PRS &lt;- rep(0, 20)
  ### mean PRS given truncation on PRS
  out_mean_PRS &lt;- rep(0, 20)
  
  for(i in 1:n_quantile){
    distribution_i &lt;- mtmvnorm(mean = mean_vec,
        sigma = sigma_mat,
        lower = c(-Inf, lower_PRS_vec[i]),
        upper = c(Inf, upper_PRS_vec[i]),
        doComputeVariance=TRUE,
        pmvnorm.algorithm=GenzBretz())
        out_mean_Y[i] &lt;- distribution_i$tmean[1]
        out_mean_PRS[i] &lt;- distribution_i$tmean[2]
        out_SD_Y[i] &lt;- sqrt(distribution_i$tvar[1,1])
        out_SD_PRS[i] &lt;- sqrt(distribution_i$tvar[2,2])
        out_cov_Y_PRS[i] &lt;- distribution_i$tvar[1,2]
  }
  
  out&lt;-data.frame(q=1:n_quantile,
             q_min=lower_PRS_vec,
             q_max=upper_PRS_vec,
             x_mean=out_mean_Y,
             x_sd=out_SD_Y)
  
  return(out)

  out_mean_Y
  out_SD_Y
  
  out_mean_PRS
  out_SD_PRS
  out_cov_Y_PRS
}

library(tmvtnorm)

# Create alternative of script that doesn&#39;t require simulation
mean_sd_quant.f &lt;- function(PRS_R2=0.641, Outcome_mean=1, Outcome_sd=1, n_quantile=20){
  ### PRS quantiles with a continuous phenotype (Y)
  library(tmvtnorm)
  ###
  E_PRS = 0
  SD_PRS = sqrt(1)
  E_phenotype = Outcome_mean
  SD_phenotype = Outcome_sd 
  n_quantile=20
  
  by_quant&lt;-1/(n_quantile)
  PRS_quantile_bounds &lt;- qnorm(p=seq(0, 1, by=by_quant), mean= E_PRS, sd= SD_PRS)
  lower_PRS_vec &lt;- PRS_quantile_bounds[1:n_quantile]
  upper_PRS_vec &lt;- PRS_quantile_bounds[2:(n_quantile+1)]
  
  mean_vec &lt;- c(E_phenotype, E_PRS)
  sigma_mat &lt;- matrix(sqrt(PRS_R2)*SD_phenotype*SD_PRS, nrow=2, ncol=2)
  sigma_mat[1,1] &lt;- SD_phenotype^2
  sigma_mat[2,2] &lt;- SD_PRS^2
  
  ### mean of phenotype within the truncated PRS distribution
  out_mean_Y &lt;- rep(0, 20)
  ### SD of phenotype within the truncated PRS distribution
  out_SD_Y &lt;- rep(0, 20)
  ### cov of Y and PRS given truncation on PRS
  out_cov_Y_PRS &lt;- rep(0, 20)
  ### SD of PRS given truncation on PRS
  out_SD_PRS &lt;- rep(0, 20)
  ### mean PRS given truncation on PRS
  out_mean_PRS &lt;- rep(0, 20)
  
  for(i in 1:n_quantile){
    distribution_i &lt;- mtmvnorm(mean = mean_vec,
        sigma = sigma_mat,
        lower = c(-Inf, lower_PRS_vec[i]),
        upper = c(Inf, upper_PRS_vec[i]),
        doComputeVariance=TRUE,
        pmvnorm.algorithm=GenzBretz())
        out_mean_Y[i] &lt;- distribution_i$tmean[1]
        out_mean_PRS[i] &lt;- distribution_i$tmean[2]
        out_SD_Y[i] &lt;- sqrt(distribution_i$tvar[1,1])
        out_SD_PRS[i] &lt;- sqrt(distribution_i$tvar[2,2])
        out_cov_Y_PRS[i] &lt;- distribution_i$tvar[1,2]
  }
  
  out&lt;-data.frame(q=1:n_quantile,
             q_min=lower_PRS_vec,
             q_max=upper_PRS_vec,
             x_mean=out_mean_Y,
             x_sd=out_SD_Y)
  
  return(out)

  out_mean_Y
  out_SD_Y
  
  out_mean_PRS
  out_SD_PRS
  out_cov_Y_PRS
}

pdf(&#39;~/comp_stand.pdf&#39;)
for(i in seq(0.05, 0.95, 0.05)){
sim_res&lt;-PRS_abs_quant2(PRS_R2 = i, Outcome_mean = 0, Outcome_sd = 1, n_quantile = 20)
nosim_res&lt;-mean_sd_quant.f(PRS_R2 = i, Outcome_mean = 0, Outcome_sd = 1, n_quantile = 20)

plot(sim_res$x_mean,nosim_res$x_mean, main=paste0(&#39;R2 = &#39;,i))
abline(coef = c(0,1))
}
dev.off()

pdf(&#39;~/comp_centre.pdf&#39;)
for(i in seq(0.05, 0.95, 0.05)){
sim_res&lt;-PRS_abs_quant2(PRS_R2 = i, Outcome_mean = 0, Outcome_sd = 2, n_quantile = 20)
nosim_res&lt;-mean_sd_quant.f(PRS_R2 = i, Outcome_mean = 0, Outcome_sd = 2, n_quantile = 20)

plot(sim_res$x_mean,nosim_res$x_mean, main=paste0(&#39;R2 = &#39;,i))
abline(coef = c(0,1))

plot(sim_res$x_sd,nosim_res$x_sd, main=paste0(&#39;R2 = &#39;,i))
abline(coef = c(0,1))

}
dev.off()

pdf(&#39;~/comp_scaled.pdf&#39;)
for(i in seq(0.05, 0.95, 0.05)){
sim_res&lt;-PRS_abs_quant2(PRS_R2 = i, Outcome_mean = 100, Outcome_sd = 1, n_quantile = 20)
nosim_res&lt;-mean_sd_quant.f(PRS_R2 = i, Outcome_mean = 100, Outcome_sd = 1, n_quantile = 20)

plot(sim_res$x_mean,nosim_res$x_mean, main=paste0(&#39;R2 = &#39;,i))
abline(coef = c(0,1))
}
dev.off()</code></pre>
</details>
<hr />
<div id="validate-conversion-1" class="section level4">
<h4><span class="header-section-number">1.1.2.1</span> Validate conversion</h4>
<p>Calculate reference-standardised polygenic scores within UK Biobank for a range of continuous phenotypes. Estimate the R2 of the polygenic scores in UKB. Compare measured and estimated absolute meana and sd per PRS quantile. Use the PRScs fully baysian (pseudovalidation) polygenic scores, as this method provides a single score with good relative performance compared to other approaches.</p>
<p>Reference-standardised polygenic scores have already been calculated in UKB for the PRS methods comparison study, and the R2 has already been estimated. Read in polygenic scores and observed phenotype for UKB, measure phenotype mean and sd per PRS quantile, and then estimate measure phenotype mean and sd per PRS quantile.</p>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)
library(e1071)

source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Target_scoring.config&#39;)

gwas&lt;-c(&#39;COLL01&#39;,&#39;HEIG03&#39;,&#39;BODY04&#39;)
pheno&lt;-c(&#39;Intelligence&#39;,&#39;Height&#39;,&#39;BMI&#39;)

n_quant&lt;-20

files&lt;-data.frame(pheno,gwas)

# Create function
mean_sd_quant.f &lt;- function(PRS_R2=0.641, Outcome_mean=1, Outcome_sd=1, n_quantile=20){
  ### PRS quantiles with a continuous phenotype (Y)
  library(tmvtnorm)
  ###
  E_PRS = 0
  SD_PRS = sqrt(1)
  E_phenotype = Outcome_mean
  SD_phenotype = Outcome_sd 

  by_quant&lt;-1/(n_quantile)
  PRS_quantile_bounds &lt;- qnorm(p=seq(0, 1, by=by_quant), mean= E_PRS, sd= SD_PRS)
  lower_PRS_vec &lt;- PRS_quantile_bounds[1:n_quantile]
  upper_PRS_vec &lt;- PRS_quantile_bounds[2:(n_quantile+1)]
  
  mean_vec &lt;- c(E_phenotype, E_PRS)
  sigma_mat &lt;- matrix(sqrt(PRS_R2)*SD_phenotype*SD_PRS, nrow=2, ncol=2)
  sigma_mat[1,1] &lt;- SD_phenotype^2
  sigma_mat[2,2] &lt;- SD_PRS^2
  
  ### mean of phenotype within the truncated PRS distribution
  out_mean_Y &lt;- rep(0, 20)
  ### SD of phenotype within the truncated PRS distribution
  out_SD_Y &lt;- rep(0, 20)
  ### cov of Y and PRS given truncation on PRS
  out_cov_Y_PRS &lt;- rep(0, 20)
  ### SD of PRS given truncation on PRS
  out_SD_PRS &lt;- rep(0, 20)
  ### mean PRS given truncation on PRS
  out_mean_PRS &lt;- rep(0, 20)
  
  for(i in 1:n_quantile){
    distribution_i &lt;- mtmvnorm(mean = mean_vec,
        sigma = sigma_mat,
        lower = c(-Inf, lower_PRS_vec[i]),
        upper = c(Inf, upper_PRS_vec[i]),
        doComputeVariance=TRUE,
        pmvnorm.algorithm=GenzBretz())
        out_mean_Y[i] &lt;- distribution_i$tmean[1]
        out_mean_PRS[i] &lt;- distribution_i$tmean[2]
        out_SD_Y[i] &lt;- sqrt(distribution_i$tvar[1,1])
        out_SD_PRS[i] &lt;- sqrt(distribution_i$tvar[2,2])
        out_cov_Y_PRS[i] &lt;- distribution_i$tvar[1,2]
  }
  
  out&lt;-data.frame(q=1:n_quantile,
             q_min=lower_PRS_vec,
             q_max=upper_PRS_vec,
             x_mean=out_mean_Y,
             x_sd=out_SD_Y)
  
  return(out)

  out_mean_Y
  out_SD_Y
  
  out_mean_PRS
  out_SD_PRS
  out_cov_Y_PRS
}

# Run analysis for each phenotype
res_all&lt;-NULL
plots_all&lt;-list()
cor_res&lt;-NULL

for(i in 1:dim(files)[1]){
  # Read in pheno and prs data, and merge
  pheno_i&lt;-fread(paste0(UKBB_output,&#39;/Phenotype/PRS_comp_subset/UKBB.&#39;,files$pheno[i],&#39;.txt&#39;))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  prs_i&lt;-fread(paste0(UKBB_output,&#39;/PRS_for_comparison/1KG_ref/PRScs/&#39;,files$gwas[i],&#39;/UKBB.subset.w_hm3.&#39;,files$gwas[i],&#39;.PRScs_profiles&#39;))
  prs_i&lt;-prs_i[,c(&#39;FID&#39;,&#39;IID&#39;,paste0(files$gwas[i], &#39;_phiauto&#39;)), with=F]
  names(prs_i)[3]&lt;-&#39;prs&#39;

  pheno_prs&lt;-merge(pheno_i, prs_i, by=c(&#39;FID&#39;,&#39;IID&#39;))
  
  # Read in AUC for PRS
  assoc&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,files$pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,files$gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))
  prs_r2&lt;-assoc[grepl(&#39;phiauto&#39;, assoc$Predictor),]$Obs_R2
  
  # Assign individuals to observed PRS quantiles
  obs_quant&lt;-quantile(pheno_prs$prs, prob = seq(0, 1, length = n_quant+1))
  pheno_prs$obs_quant&lt;-as.numeric(cut( pheno_prs$prs, obs_quant, include.lowest = T))

  # Calculate mean and SD of each quantile that are cases
  obs_dist&lt;-NULL
  for(k in 1:n_quant){
    obs_dist&lt;-rbind(obs_dist, data.frame(Phenotype=files$pheno[i],
                                     Type=&#39;Observed&#39;,
                                     Quantile=k,
                                     q_min=obs_quant[k],
                                     q_max=obs_quant[k+1],
                                     x_mean=mean(pheno_prs$pheno[pheno_prs$obs_quant == k]),
                                     x_sd=sd(pheno_prs$pheno[pheno_prs$obs_quant == k])))
  }
  
  # Assign individuals to estimated PRS quantiles
  est_dist&lt;-mean_sd_quant.f(PRS_R2 = prs_r2, Outcome_mean=mean(pheno_prs$pheno), Outcome_sd=sd(pheno_prs$pheno), n_quantile = n_quant)
  est_dist$q&lt;-NULL
  est_dist&lt;-data.frame(Phenotype=files$pheno[i],Type=&#39;Estimated&#39;,Quantile=1:n_quant, est_dist)
  est_quant&lt;-sort(unique(c(est_dist$q_min, est_dist$q_max)))
  pheno_prs$est_quant&lt;-as.numeric(cut( pheno_prs$prs, est_quant, include.lowest = T))

  quant_comp&lt;-rbind(obs_dist, est_dist)
  
  tmp&lt;-cor.test(obs_dist$x_mean, est_dist$x_mean)
  
  cor_res&lt;-rbind(cor_res,data.frame(Phenotype=files$pheno[i],
                                    Cor_mean=tmp$estimate,
                                    Cor_mean_Low95CI=tmp$conf.int[1],
                                    Cor_mean_High95CI=tmp$conf.int[2],
                                    PercDiff_sd_mean=mean(abs(est_dist$x_sd-obs_dist$x_sd)/obs_dist$x_sd),
                                    N=length(pheno_prs$pheno),
                                    Skewness=skewness(pheno_prs$pheno)))

  res_all&lt;-rbind(res_all, quant_comp)
  
  library(ggplot2)
  library(cowplot)
  
  plots_all[[i]]&lt;-ggplot(quant_comp, aes(x=Quantile, y=x_mean, colour=Type)) +
    geom_point(stat=&quot;identity&quot;, position=position_dodge(.9), alpha=0.8, shape=18) +
    geom_errorbar(aes(ymin=x_mean-x_sd, ymax=x_mean+x_sd), width=.2, position=position_dodge(.9), alpha=0.8) +
    labs(x=&#39;PRS quantile&#39;, y=paste0(pheno[i], &#39; mean and SD&#39;), title=files$pheno[i], colour=&#39;Method&#39;) +
    theme_cowplot(12)
  
}

png(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/Mean_SD_Comp.png&#39;), units=&#39;px&#39;, res=300, width=2000, height=1500)
  plot_grid(plotlist=plots_all, ncol = 2)
dev.off()

write.csv(cor_res, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/Mean_SD_Comp.csv&#39;, row.names=F, quote=F)</code></pre>
</details>
<details>
<p><summary>Show results</summary> <img src="Images/Validating_ImputeMe_Ideas/Mean_SD_Comp.png" alt="Observed-Estimated Proportion of Cases within Polygenic Score Quantiles" /></p>
<table>
<caption>Correlation between observed and estimated mean and standar deviation of outcome within polygenic score quantiles</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">Cor_mean</th>
<th align="right">Cor_mean_Low95CI</th>
<th align="right">Cor_mean_High95CI</th>
<th align="right">PercDiff_sd_mean</th>
<th align="right">N</th>
<th align="right">Skewness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intelligence</td>
<td align="right">0.9681484</td>
<td align="right">0.9196140</td>
<td align="right">0.9875691</td>
<td align="right">0.0139143</td>
<td align="right">50000</td>
<td align="right">0.1440539</td>
</tr>
<tr class="even">
<td align="left">Height</td>
<td align="right">0.9973953</td>
<td align="right">0.9932740</td>
<td align="right">0.9989926</td>
<td align="right">0.0156650</td>
<td align="right">49999</td>
<td align="right">0.1168512</td>
</tr>
<tr class="odd">
<td align="left">BMI</td>
<td align="right">0.9983971</td>
<td align="right">0.9958577</td>
<td align="right">0.9993802</td>
<td align="right">0.0574380</td>
<td align="right">49999</td>
<td align="right">0.5915969</td>
</tr>
</tbody>
</table>
<p>Median Cor. of means = 0.9973953; Mean Cor. of means = 0.9879803; Min. Cor. of means = 0.9681484; Max. Cor. of means = 0.9983971</p>
<p>Median mean %diff of SD = 0.01566497; Mean mean %diff of SD = 0.02900575; Min. mean %diff of SD = 0.01391425; Max. mean %diff of SD = 0.05743802</p>
</details>
<hr />
</div>
</div>
</div>
<div id="interactive-tool-for-converting-prs-into-absolute-risk-estimates" class="section level2">
<h2><span class="header-section-number">1.2</span> Interactive tool for converting PRS into absolute risk estimates</h2>
<hr />
<div id="binary-outcomes-1" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Binary outcomes</h3>
<details>
<p><summary>Show tool</summary></p>
<iframe height="1100" width="120%" frameborder="no" src="https://opain.shinyapps.io/risk_visualiser/">
</iframe>
</details>
<hr />
</div>
<div id="quantitative-outcomes" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Quantitative outcomes</h3>
<details>
<p><summary>Show tool</summary></p>
<iframe height="800" width="120%" frameborder="no" src="https://opain.shinyapps.io/trait_visualiser/">
</iframe>
</details>
<hr />
<p>Both these conversions require estimates of AUC/R2 by the polygenic score. However, this information is often not known for a given polygenic score so approaches to estimate the variance explained by a polygenic scores using summary statistics is required.</p>
<hr />
</div>
</div>
</div>
<div id="estimating-aucr2-by-polygenic-score" class="section level1">
<h1><span class="header-section-number">2</span> Estimating AUC/R2 by polygenic score</h1>
<p>Here we will use an approach using LD-score regression to estimate the SNP-based heritability of the GWAS phenotype, and subsequent use of AVENGEME to estimate the variance explained by the polygenic score given the heritability of the phenotype and the sample size of the GWAS.</p>
<p>We validate this method by comparison to observed variance explained measurements in UK Biobank. We use two approaches to generate the polygenic scores:</p>
<ul>
<li><p>Using a 10-fold cross-validation GWAS-polygenic scoring procedure to avoid overfitting. This approach ensure the genetic correlation between training and target samples is 1.</p></li>
<li><p>Using external GWAS summary statistics to generate polygenic scores. This approach is more realistic.</p></li>
</ul>
<hr />
<div id="validation-using-external-gwas" class="section level2">
<h2><span class="header-section-number">2.1</span> Validation using External GWAS</h2>
<p>Estimate the AUC/R2 of polygenic scores from external GWAS, and compare to observed estimates.</p>
<hr />
<div id="ldscavengeme" class="section level3">
<h3><span class="header-section-number">2.1.1</span> LDSC/AVENGEME</h3>
<hr />
<div id="ldsc" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> LDSC</h4>
<details>
<p><summary>Show results</summary></p>
<pre class="r"><code># Create version of HapMap3 SNP list that doesn&#39;t contain the MHC region
library(data.table)
source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config&#39;)

hm3&lt;-fread(paste0(ldsc_ref,&#39;/w_hm3.snplist&#39;))

bim&lt;-fread(&#39;/scratch/groups/biomarkers-brc-mh/Reference_data/1KG_Phase3/PLINK/all_phase3.chr6.bim&#39;)
bim_mhc&lt;-bim[bim$V1 == 6 &amp; bim$V4 &gt; 28e6 &amp; bim$V4 &lt; 34e6,]

hm3_nomhc&lt;-hm3[!(hm3$SNP %in% bim_mhc$V2),]

write.table(hm3_nomhc, paste0(ldsc_ref,&#39;/w_hm3_nomhc.snplist&#39;), col.names = T, row.names = F, quote = F)


########

# Run LDSC for binary outcomes
library(data.table)
source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config&#39;)

pheno=c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
gwas=c(&#39;DEPR06&#39;,&#39;DIAB05&#39;,&#39;COAD01&#39;,&#39;CROH01&#39;,&#39;SCLE03&#39;,&#39;RHEU02&#39;,&#39;BRCA01&#39;,&#39;PRCA01&#39;)
pop_prev=c(0.15,0.05,0.03,0.013,0.00164,0.005,0.125,0.125)
samp_prev=c(0.28,0.168,0.33,0.285,0.36,0.246,0.537,0.564)

# Munge the sumstats
for(i in 1:length(pheno)){
  system(paste0(munge_sumstats,&#39; --sumstats &#39;,gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.cleaned.gz --merge-alleles &#39;,ldsc_ref,&#39;/w_hm3_nomhc.snplist --out &#39;, gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.munged&#39;))
}

# Estimate heritability
for(i in 1:length(pheno)){
  system(paste0(ldsc,&#39; --h2 &#39;,gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.munged.sumstats.gz --ref-ld-chr &#39;,ldsc_ref,&#39;/ --w-ld-chr &#39;,ldsc_ref,&#39;/ --out /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/&#39;,pheno[i],&#39;_h2 --samp-prev &#39;,samp_prev[i],&#39; --pop-prev &#39;,pop_prev[i]))
}

h2_all&lt;-NULL
for(i in 1:length(pheno)){
  ldsc_log&lt;-read.table(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/&#39;,pheno[i],&#39;_h2.log&#39;), header=F, sep=&#39;&amp;&#39;)
  ldsc_h2&lt;-ldsc_log[grepl(&#39;Total Liability scale h2&#39;, ldsc_log$V1),]
  ldsc_h2&lt;-gsub(&#39;Total Liability scale h2: &#39;,&#39;&#39;, ldsc_h2)
  ldsc_h2_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;, ldsc_h2))
  ldsc_h2_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;,&#39;&#39;, ldsc_h2)))
  
  ldsc_int&lt;-ldsc_log[grepl(&#39;Intercept: &#39;, ldsc_log$V1),]
  ldsc_int&lt;-gsub(&#39;Intercept: &#39;,&#39;&#39;, ldsc_int)
  ldsc_int_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;, ldsc_int))
  ldsc_int_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;,&#39;&#39;, ldsc_int)))

  munged&lt;-fread(cmd=paste0(&#39;zcat &#39;,gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.munged.sumstats.gz&#39;))
  N&lt;-median(munged$N, na.rm = T)
  
  h2_all&lt;-rbind(h2_all, data.frame(GWAS=gwas[i],
                                   Phenotype=pheno[i],
                                   h2=ldsc_h2_est,
                                   h2_se=ldsc_h2_se,
                                   int=ldsc_int_est,
                                   int_se=ldsc_int_se,
                                   pop_prev=pop_prev[i],
                                   samp_prev=samp_prev[i],
                                   N=N))
}

write.csv(h2_all, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/ldsc_h2.csv&#39;, row.names=F, quote=F)

#####
# Continuous outcomes
#####
library(data.table)
source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config&#39;)

gwas&lt;-c(&#39;COLL01&#39;,&#39;HEIG03&#39;,&#39;BODY04&#39;)
pheno=c(&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;)

# Munge the sumstats
for(i in 1:length(pheno)){
  system(paste0(munge_sumstats,&#39; --sumstats &#39;,gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.cleaned.gz --merge-alleles &#39;,ldsc_ref,&#39;/w_hm3_nomhc.snplist --out &#39;, gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.munged&#39;))
}

# Estimate heritability
for(i in 1:length(pheno)){
  system(paste0(ldsc,&#39; --h2 &#39;,gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.munged.sumstats.gz --ref-ld-chr &#39;,ldsc_ref,&#39;/ --w-ld-chr &#39;,ldsc_ref,&#39;/ --out /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/&#39;,pheno[i],&#39;_h2_obs&#39;))
}

h2_all&lt;-NULL
for(i in 1:length(pheno)){
  ldsc_log&lt;-read.table(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/&#39;,pheno[i],&#39;_h2_obs.log&#39;), header=F, sep=&#39;&amp;&#39;)
  ldsc_h2&lt;-ldsc_log[grepl(&#39;Total Observed scale h2&#39;, ldsc_log$V1),]
  ldsc_h2&lt;-gsub(&#39;Total Observed scale h2: &#39;,&#39;&#39;, ldsc_h2)
  ldsc_h2_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;, ldsc_h2))
  ldsc_h2_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;,&#39;&#39;, ldsc_h2)))

  ldsc_int&lt;-ldsc_log[grepl(&#39;Intercept: &#39;, ldsc_log$V1),]
  ldsc_int&lt;-gsub(&#39;Intercept: &#39;,&#39;&#39;, ldsc_int)
  ldsc_int_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;, ldsc_int))
  ldsc_int_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;,&#39;&#39;, ldsc_int)))
  
  munged&lt;-fread(cmd=paste0(&#39;zcat &#39;,gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.munged.sumstats.gz&#39;))
  N&lt;-median(munged$N, na.rm = T)
  
  h2_all&lt;-rbind(h2_all, data.frame(GWAS=gwas[i],
                                   Phenotype=pheno[i],
                                   h2=ldsc_h2_est,
                                   h2_se=ldsc_h2_se,
                                   int=ldsc_int_est,
                                   int_se=ldsc_int_se,
                                   N=N))
}

write.csv(h2_all, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/ldsc_h2_obs.csv&#39;, row.names=F, quote=F)</code></pre>
</details>
<hr />
</div>
<div id="avengeme" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> AVENGEME</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(avengeme)
library(ggplot2)
library(cowplot)

pheno=c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
gwas=c(&#39;DEPR06&#39;,&#39;DIAB05&#39;,&#39;COAD01&#39;,&#39;CROH01&#39;,&#39;SCLE03&#39;,&#39;RHEU02&#39;,&#39;BRCA01&#39;,&#39;PRCA01&#39;)

#####
# Estimate using the liability h2
#####
h2&lt;-fread(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/ldsc_h2.csv&#39;)

AUC_pred&lt;-NULL
AVENGEME_plots&lt;-list()
for(i in 1:length(pheno)){
  
  # Read in observed AUC estimates
  assoc&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))
  
  # Estimate heritability using AVENGEME
  pheno_nsnp&lt;-NULL
  tmp&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/1KG/Phase3/Score_files_for_polygenic/pt_clump/&#39;,gwas[i],&#39;/1KGPhase3.w_hm3.&#39;,gwas[i],&#39;.NSNP_per_pT&#39;))
  pheno_nsnp&lt;-tmp$NSNP[length(tmp$NSNP)]

  pheno_res_pTclump&lt;-assoc[grepl(&#39;PredFile1&#39;,assoc$Predictor),]
  pT&lt;-as.numeric(gsub(&#39;e.&#39;,&#39;e-&#39;,gsub(&#39;.*_&#39;,&#39;&#39;,pheno_res_pTclump$Predictor)))
  
  tmp2&lt;-estimatePolygenicModel(p=pheno_res_pTclump$Estimate/pheno_res_pTclump$SE, 
                           nsnp=pheno_nsnp, 
                           n=c(h2$N[h2$Phenotype == pheno[i]], pheno_res_pTclump$N[1]), 
                           pupper = c(0,pT),
                           binary = c(T, T), 
                           prevalence = h2$pop_prev[h2$Phenotype == pheno[i]], 
                           sampling = c(h2$samp_prev[h2$Phenotype == pheno[i]], (pheno_res_pTclump$Ncas/pheno_res_pTclump$N)[1]), 
                           fixvg2pi02 = T,
                           alpha = 0.05)

  # Run AVENGEME predicted AUC using LDSC heritability
  avengeme_res_ldsc&lt;-NULL
  for(pT in c(5e-8,1e-6,1e-5,1e-4,1e-3,0.01,0.1,0.2,0.3,0.4,0.5,1)){
    for(pi0 in seq(0.92,0.98,0.02)){
        tmp&lt;-polygenescore(nsnp=pheno_nsnp, n=h2$N[h2$Phenotype == pheno[i]], vg1 = h2$h2[h2$Phenotype == pheno[i]], pupper = c(0, pT), pi0 = pi0, nested = TRUE, weighted = TRUE, binary = T, prevalence = h2$pop_prev[h2$Phenotype == pheno[i]], sampling = h2$samp_prev[h2$Phenotype == pheno[i]])
        
      avengeme_res_ldsc&lt;-rbind(avengeme_res_ldsc, data.frame(pT=pT,
                                                   pi0=pi0,
                                                   AUC=tmp$AUC))
    }
  }

  # Run AVENGEME predicted AUC using AVENGEME heritability
  avengeme_res_avenge&lt;-NULL
  for(pT in c(5e-8,1e-6,1e-5,1e-4,1e-3,0.01,0.1,0.2,0.3,0.4,0.5,1)){
    for(pi0 in seq(0.92,0.98,0.02)){
        tmp&lt;-polygenescore(nsnp=pheno_nsnp, n=h2$N[h2$Phenotype == pheno[i]], vg1 = tmp2$vg[1], pupper = c(0, pT), pi0 = pi0, nested = TRUE, weighted = TRUE, binary = T, prevalence = h2$pop_prev[h2$Phenotype == pheno[i]], sampling = h2$samp_prev[h2$Phenotype == pheno[i]])
        
      avengeme_res_avenge&lt;-rbind(avengeme_res_avenge, data.frame(pT=pT,
                                                   pi0=pi0,
                                                   AUC=tmp$AUC))
    }
  }

  prs_auc_pt1&lt;-assoc[grepl(paste0(gwas[i],&#39;_1$&#39;), assoc$Predictor),]$AUC
  prscs_auc&lt;-assoc[grepl(&#39;phiauto&#39;, assoc$Predictor),]$AUC

  obs_res&lt;-data.frame(Method=c(&#39;pT+clump&#39;,&#39;PRScs&#39;),
                      AUC=c(prs_auc_pt1, prscs_auc))
  
  AVENGEME_plots[[paste0(i,&#39;_LDSC&#39;)]]&lt;-ggplot(avengeme_res_ldsc, aes(x=factor(pT), y=AUC, group=factor(pi0),colour=factor(pi0))) +
    geom_point() +
    geom_line() +
    theme_cowplot(12) +
    ylim(0.5,0.8) +
    labs(x=&#39;pT&#39;, y=&#39;AUC&#39;, title=paste0(pheno[i],&#39;: LDSC h2&#39;), colour=&#39;pi0&#39;) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    geom_hline(data=obs_res, aes(yintercept=AUC, linetype=Method))
  
  AVENGEME_plots[[paste0(i,&#39;_AVENG&#39;)]]&lt;-ggplot(avengeme_res_avenge, aes(x=factor(pT), y=AUC, group=factor(pi0),colour=factor(pi0))) +
    geom_point() +
    geom_line() +
    theme_cowplot(12) +
    ylim(0.5,0.8) +
    labs(x=&#39;pT&#39;, y=&#39;AUC&#39;, title=paste0(pheno[i],&#39;: AVENGEME h2&#39;), colour=&#39;pi0&#39;) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    geom_hline(data=obs_res, aes(yintercept=AUC, linetype=Method))

  avengeme_res_ldsc_best_94&lt;-max(avengeme_res_ldsc$AUC[as.character(avengeme_res_ldsc$pi0) == 0.94])
  
  avengeme_res_avenge_best_94&lt;-max(avengeme_res_avenge$AUC[as.character(avengeme_res_avenge$pi0) == 0.94])

  AUC_pred&lt;-rbind(AUC_pred, data.frame(Phenotype=pheno[i],
                                         N=h2$N[h2$Phenotype == pheno[i]],
                                         vg1=h2$h2[h2$Phenotype == pheno[i]],
                                       Intercept=h2$int[h2$Phenotype == pheno[i]],
                                         prevalence=h2$pop_prev[h2$Phenotype == pheno[i]],
                                         sampling=h2$samp_prev[h2$Phenotype == pheno[i]],
                                         NSNP=pheno_nsnp,
                                         pred_AUC_LDSC=avengeme_res_ldsc_best_94,
                                         pred_AUC_AVENG=avengeme_res_avenge_best_94,
                                         pTclump_pt1_AUC=prs_auc_pt1,
                                         PRScs_AUC=prscs_auc))
}

png(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/AVENGEME_AUC.png&#39;), units=&#39;px&#39;, res=300, width=2200, height=6000)
  plot_grid(plotlist=AVENGEME_plots, ncol = 2)
dev.off()

write.csv(AUC_pred, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/AVENGEME_AUC.csv&#39;, row.names=F, quote = F)

# The LDSC intercept is below 1 for some GWAS leading to inaccurate h2 estimates
# When using AVENGEME estimates of h2, the optimal pT assuming a p0 of 0.94 is fairly close to the observed PRScs AUC. Its accuracy depends on how close the observed pi0 of these outcomes is to 0.94.
# Integration of polygenicity estimates based on GWAS summary statistics would be a future direction.

###########
# Estimate for continuous outcomes
###########

library(avengeme)
library(ggplot2)
library(cowplot)

pheno=c(&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;)
gwas=c(&#39;COLL01&#39;,&#39;BODY04&#39;,&#39;HEIG03&#39;)

h2&lt;-fread(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/ldsc_h2_obs.csv&#39;)

R2_pred&lt;-NULL
AVENGEME_plots&lt;-list()
for(i in 1:length(pheno)){
  
  # Read in observed R2 estimates
  assoc&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))
  
  # Estimate heritability using AVENGEME
  pheno_nsnp&lt;-NULL
  tmp&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/1KG/Phase3/Score_files_for_polygenic/pt_clump/&#39;,gwas[i],&#39;/1KGPhase3.w_hm3.&#39;,gwas[i],&#39;.NSNP_per_pT&#39;))
  pheno_nsnp&lt;-tmp$NSNP[length(tmp$NSNP)]

  pheno_res_pTclump&lt;-assoc[grepl(&#39;PredFile1&#39;,assoc$Predictor),]
  pT&lt;-as.numeric(gsub(&#39;e.&#39;,&#39;e-&#39;,gsub(&#39;.*_&#39;,&#39;&#39;,pheno_res_pTclump$Predictor)))
  
  tmp2&lt;-estimatePolygenicModel(p=pheno_res_pTclump$BETA/pheno_res_pTclump$SE, 
                           nsnp=pheno_nsnp, 
                           n=c(h2$N[h2$Phenotype == pheno[i]], pheno_res_pTclump$N[1]), 
                           pupper = c(0,pT),
                           binary = F, 
                           fixvg2pi02 = T,
                           alpha = 0.05)

  # Run AVENGEME predicted R2 using LDSC heritability
  avengeme_res_ldsc&lt;-NULL
  for(pT in c(5e-8,1e-6,1e-5,1e-4,1e-3,0.01,0.1,0.2,0.3,0.4,0.5,1)){
    for(pi0 in seq(0.92,0.98,0.02)){
        tmp&lt;-polygenescore(nsnp=pheno_nsnp, n=h2$N[h2$Phenotype == pheno[i]], vg1 = h2$h2[h2$Phenotype == pheno[i]], pupper = c(0, pT), pi0 = pi0, nested = TRUE, weighted = TRUE, binary = F)
        
      avengeme_res_ldsc&lt;-rbind(avengeme_res_ldsc, data.frame(pT=pT,
                                                   pi0=pi0,
                                                   R2=tmp$R2))
    }
  }

  # Run AVENGEME predicted R2 using AVENGEME heritability
  avengeme_res_avenge&lt;-NULL
  for(pT in c(5e-8,1e-6,1e-5,1e-4,1e-3,0.01,0.1,0.2,0.3,0.4,0.5,1)){
    for(pi0 in seq(0.92,0.98,0.02)){
        tmp&lt;-polygenescore(nsnp=pheno_nsnp, n=h2$N[h2$Phenotype == pheno[i]], vg1 = tmp2$vg[1], pupper = c(0, pT), pi0 = pi0, nested = TRUE, weighted = TRUE, binary = F)
        
      avengeme_res_avenge&lt;-rbind(avengeme_res_avenge, data.frame(pT=pT,
                                                   pi0=pi0,
                                                   R2=tmp$R2))
    }
  }

  prs_R2_pt1&lt;-assoc[grepl(paste0(gwas[i],&#39;_1$&#39;), assoc$Predictor),]$Obs_R2
  prscs_R2&lt;-assoc[grepl(&#39;phiauto&#39;, assoc$Predictor),]$Obs_R2

  obs_res&lt;-data.frame(Method=c(&#39;pT+clump&#39;,&#39;PRScs&#39;),
                      R2=c(prs_R2_pt1, prscs_R2))
  
  y_limit&lt;-max(c(avengeme_res_ldsc$R2, avengeme_res_avenge$R2))
  
  AVENGEME_plots[[paste0(i,&#39;_LDSC&#39;)]]&lt;-ggplot(avengeme_res_ldsc, aes(x=factor(pT), y=R2, group=factor(pi0),colour=factor(pi0))) +
    geom_point() +
    geom_line() +
    theme_cowplot(12) +
    ylim(0,y_limit) +
    labs(x=&#39;pT&#39;, y=&#39;R2&#39;, title=paste0(pheno[i],&#39;: LDSC h2&#39;), colour=&#39;pi0&#39;) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    geom_hline(data=obs_res, aes(yintercept=R2, linetype=Method))
  
  AVENGEME_plots[[paste0(i,&#39;_AVENG&#39;)]]&lt;-ggplot(avengeme_res_avenge, aes(x=factor(pT), y=R2, group=factor(pi0),colour=factor(pi0))) +
    geom_point() +
    geom_line() +
    theme_cowplot(12) +
    ylim(0,y_limit) +
    labs(x=&#39;pT&#39;, y=&#39;R2&#39;, title=paste0(pheno[i],&#39;: AVENGEME h2&#39;), colour=&#39;pi0&#39;) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    geom_hline(data=obs_res, aes(yintercept=R2, linetype=Method))

  avengeme_res_ldsc_best_94&lt;-max(avengeme_res_ldsc$R2[as.character(avengeme_res_ldsc$pi0) == 0.94])
  
  avengeme_res_avenge_best_94&lt;-max(avengeme_res_avenge$R2[as.character(avengeme_res_avenge$pi0) == 0.94])

  R2_pred&lt;-rbind(R2_pred, data.frame(Phenotype=pheno[i],
                                         N=h2$N[h2$Phenotype == pheno[i]],
                                         vg1=h2$h2[h2$Phenotype == pheno[i]],
                                       Intercept=h2$int[h2$Phenotype == pheno[i]],
                                         NSNP=pheno_nsnp,
                                         pred_R2_LDSC=avengeme_res_ldsc_best_94,
                                         pred_R2_AVENG=avengeme_res_avenge_best_94,
                                         pTclump_pt1_R2=prs_R2_pt1,
                                         PRScs_R2=prscs_R2))
}

png(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/AVENGEME_R2.png&#39;), units=&#39;px&#39;, res=300, width=2200, height=3000)
  plot_grid(plotlist=AVENGEME_plots, ncol = 2)
dev.off()

write.csv(R2_pred, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/AVENGEME_R2.csv&#39;, row.names=F, quote = F)</code></pre>
</details>
<details>
<p><summary>Show results</summary> <img src="Images/Validating_ImputeMe_Ideas/AVENGEME_AUC.png" alt="Estimates of AUC using AVENGEME/LDSC" /></p>
<table>
<caption>Etimation of PRS AUC using LDSC and AVENGEME approach</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">N</th>
<th align="right">vg1</th>
<th align="right">Intercept</th>
<th align="right">prevalence</th>
<th align="right">sampling</th>
<th align="right">NSNP</th>
<th align="right">pred_AUC_LDSC</th>
<th align="right">pred_AUC_AVENG</th>
<th align="right">pTclump_pt1_AUC</th>
<th align="right">PRScs_AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Depression</td>
<td align="right">431394</td>
<td align="right">0.0836</td>
<td align="right">1.0085</td>
<td align="right">0.15000</td>
<td align="right">0.280</td>
<td align="right">105913</td>
<td align="right">0.5952020</td>
<td align="right">0.5928064</td>
<td align="right">0.5666770</td>
<td align="right">0.5783668</td>
</tr>
<tr class="even">
<td align="left">T2D</td>
<td align="right">152599</td>
<td align="right">0.1182</td>
<td align="right">0.9990</td>
<td align="right">0.05000</td>
<td align="right">0.168</td>
<td align="right">120474</td>
<td align="right">0.5795090</td>
<td align="right">0.5985747</td>
<td align="right">0.5920100</td>
<td align="right">0.6401091</td>
</tr>
<tr class="odd">
<td align="left">CAD</td>
<td align="right">184305</td>
<td align="right">0.0549</td>
<td align="right">0.8867</td>
<td align="right">0.03000</td>
<td align="right">0.330</td>
<td align="right">121583</td>
<td align="right">0.5592963</td>
<td align="right">0.5902170</td>
<td align="right">0.5759404</td>
<td align="right">0.6033915</td>
</tr>
<tr class="even">
<td align="left">IBD</td>
<td align="right">20883</td>
<td align="right">0.3397</td>
<td align="right">1.0306</td>
<td align="right">0.01300</td>
<td align="right">0.285</td>
<td align="right">92044</td>
<td align="right">0.6697195</td>
<td align="right">0.5728642</td>
<td align="right">0.5816432</td>
<td align="right">0.6293128</td>
</tr>
<tr class="odd">
<td align="left">MultiScler</td>
<td align="right">27148</td>
<td align="right">0.0199</td>
<td align="right">1.0622</td>
<td align="right">0.00164</td>
<td align="right">0.360</td>
<td align="right">89724</td>
<td align="right">0.5153772</td>
<td align="right">0.6229348</td>
<td align="right">0.6150556</td>
<td align="right">0.6821434</td>
</tr>
<tr class="even">
<td align="left">RheuArth</td>
<td align="right">58284</td>
<td align="right">0.1427</td>
<td align="right">1.0647</td>
<td align="right">0.00500</td>
<td align="right">0.246</td>
<td align="right">112704</td>
<td align="right">0.6307344</td>
<td align="right">0.5841640</td>
<td align="right">0.5823766</td>
<td align="right">0.6501171</td>
</tr>
<tr class="odd">
<td align="left">Breast_Cancer</td>
<td align="right">228951</td>
<td align="right">0.1499</td>
<td align="right">1.1033</td>
<td align="right">0.12500</td>
<td align="right">0.537</td>
<td align="right">122330</td>
<td align="right">0.6352611</td>
<td align="right">0.6288307</td>
<td align="right">0.6016412</td>
<td align="right">0.6583790</td>
</tr>
<tr class="even">
<td align="left">Prostate_Cancer</td>
<td align="right">140254</td>
<td align="right">0.1876</td>
<td align="right">1.0810</td>
<td align="right">0.12500</td>
<td align="right">0.564</td>
<td align="right">92109</td>
<td align="right">0.6517146</td>
<td align="right">0.6732014</td>
<td align="right">0.6316156</td>
<td align="right">0.6975937</td>
</tr>
</tbody>
</table>
<div class="figure">
<img src="Images/Validating_ImputeMe_Ideas/AVENGEME_R2.png" alt="Estimates of R2 using AVENGEME/LDSC" />
<p class="caption">Estimates of R2 using AVENGEME/LDSC</p>
</div>
<table>
<caption>Etimation of PRS R2 using LDSC and AVENGEME approach</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">N</th>
<th align="right">vg1</th>
<th align="right">Intercept</th>
<th align="right">NSNP</th>
<th align="right">pred_R2_LDSC</th>
<th align="right">pred_R2_AVENG</th>
<th align="right">pTclump_pt1_R2</th>
<th align="right">PRScs_R2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intelligence</td>
<td align="right">95427</td>
<td align="right">0.1047</td>
<td align="right">1.0217</td>
<td align="right">98664</td>
<td align="right">0.0156250</td>
<td align="right">0.0088454</td>
<td align="right">0.0062422</td>
<td align="right">0.0095257</td>
</tr>
<tr class="even">
<td align="left">BMI</td>
<td align="right">252064</td>
<td align="right">0.3115</td>
<td align="right">1.3279</td>
<td align="right">110277</td>
<td align="right">0.2368235</td>
<td align="right">0.1078916</td>
<td align="right">0.0576659</td>
<td align="right">0.0796553</td>
</tr>
<tr class="odd">
<td align="left">Height</td>
<td align="right">233681</td>
<td align="right">0.1299</td>
<td align="right">0.6722</td>
<td align="right">106650</td>
<td align="right">0.0629477</td>
<td align="right">0.1100028</td>
<td align="right">0.0634522</td>
<td align="right">0.1195499</td>
</tr>
</tbody>
</table>
</details>
<hr />
</div>
</div>
<div id="gwiz" class="section level3">
<h3><span class="header-section-number">2.1.2</span> GWIZ</h3>
<p>We will calculate the AUC using GWIZ using the leave one out meta-analysis summary statistics, and then average the AUC results. This will hopefully tell us how much variance GWAS expects the LOO sumstats to explain in the left out subset.</p>
<hr />
<div id="format-gwas-summary-statistics" class="section level4">
<h4><span class="header-section-number">2.1.2.1</span> Format GWAS summary statistics</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config&#39;)

pheno=c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
gwas=c(&#39;DEPR06&#39;,&#39;DIAB05&#39;,&#39;COAD01&#39;,&#39;CROH01&#39;,&#39;SCLE03&#39;,&#39;RHEU02&#39;,&#39;BRCA01&#39;,&#39;PRCA01&#39;)
samp_prev=c(0.28,0.168,0.33,0.285,0.36,0.246,0.537,0.564)

dir.create(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/GWIZ&#39;)

for(i in 1:length(pheno)){
  # Read in GWAS sumstats
  sumstats&lt;-fread(paste0(gwas_rep_qcd,&#39;/&#39;,gwas[i],&#39;.cleaned.gz&#39;))
  sumstats$phenotype&lt;-&#39;n/a&#39;
  sumstats$dataset&lt;-&#39;n/a&#39;
  sumstats$model&lt;-&#39;additive&#39;
  
  if(sum(names(sumstats) == &#39;Ncas&#39;) == 0){
    sumstats$Ncas&lt;-round(sumstats$N*samp_prev[i])
    sumstats$Ncon&lt;-round(sumstats$N*(1-samp_prev[i]))
  }
  
  sumstats$control_size&lt;-sumstats$Ncon
  sumstats$case_size&lt;-sumstats$Ncas

  if(sum(names(sumstats) == &#39;REF.FREQ&#39;) &gt; 0){
    sumstats$FREQ&lt;-sumstats$REF.FREQ
    sumstats$REF.FREQ&lt;-NULL
  }

  if(sum(names(sumstats) == &#39;OR&#39;) == 0){
    sumstats$OR&lt;-exp(sumstats$BETA)
  }
  
  # OR and MAFs to all be corresponding to the risk allele
  sumstats$FREQ[sumstats$OR &lt; 1]&lt;-1-sumstats$FREQ[sumstats$OR &lt; 1]
  names(sumstats)[names(sumstats) == &#39;FREQ&#39;]&lt;-&#39;risk_allele_freq&#39;
  sumstats$BETA&lt;-log(sumstats$OR)
  sumstats$OR[sumstats$OR &lt; 1]&lt;-exp(-sumstats$BETA[sumstats$OR &lt; 1])
  
  # Extract LD independent variants
  ld_indep&lt;-NULL
  for(chr in 1:22){
    ld_indep&lt;-rbind(ld_indep, fread(paste0(&#39;/users/k1806347/brc_scratch/Data/1KG/Phase3/Score_files_for_polygenic/pt_clump/&#39;,gwas[i],&#39;/1KGPhase3.w_hm3.&#39;,gwas[i],&#39;.chr&#39;,chr,&#39;.range_values&#39;)))
  }
  
  # Filter genome-wide variants 
  ld_indep_sig&lt;-ld_indep[ld_indep$V2 &lt; 1e-8]
  sumstats&lt;-sumstats[(sumstats$SNP %in% ld_indep_sig$V1),]
  sumstats&lt;-sumstats[,c(&#39;phenotype&#39;,&#39;dataset&#39;,&#39;SNP&#39;,&#39;control_size&#39;,&#39;case_size&#39;,&#39;risk_allele_freq&#39;,&#39;OR&#39;,&#39;model&#39;),with=F]
  names(sumstats)[names(sumstats) == &#39;SNP&#39;]&lt;-&#39;accession&#39;
  
  sumstats$model&lt;-rep(c(&#39;recessive&#39;,&#39;dominant&#39;),dim(sumstats)[1])[1:dim(sumstats)[1]]
  
  fwrite(sumstats, paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/GWIZ/&#39;,gwas[i],&#39;.GWIZ.csv&#39;), sep=&#39;,&#39;, na=&#39;NA&#39;, quote=F)
}</code></pre>
</details>
<hr />
</div>
<div id="run-gwiz" class="section level4">
<h4><span class="header-section-number">2.1.2.2</span> Run GWIZ</h4>
<p>GWIZ is an R script. It needs to modification: - It doesn’t allow input files, software and output to be in different directories - It uses the png function which does’t work on Rosalind - It prints the contents of the GWAS sumstats repeatedly and GWIZ is an Rscript that must be modified to allow for different input and output files. I have edited the script slightly so all files don’t have be stored in the same directory, and can be easily run in parallel.</p>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code># Run GWIZ for each phenotype and subset
for gwas in $(echo DEPR06 DIAB05 COAD01 CROH01 SCLE03 RHEU02 BRCA01 PRCA01); do
    sbatch -p brc,shared --mem 10G /users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/gwizer/gwizer.R \
      --sumstats /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/GWIZ/${gwas}.GWIZ.csv \
      --gwiz /mnt/lustre/users/k1806347/Software/GWIZ-Rscript-master \
      --output /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/GWIZ/${gwas}/res_${gwas}
done</code></pre>
</details>
<hr />
</div>
<div id="compare-observed-and-estimate-auc" class="section level4">
<h4><span class="header-section-number">2.1.2.3</span> Compare observed and estimate AUC</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>pheno=c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
gwas=c(&#39;DEPR06&#39;,&#39;DIAB05&#39;,&#39;COAD01&#39;,&#39;CROH01&#39;,&#39;SCLE03&#39;,&#39;RHEU02&#39;,&#39;BRCA01&#39;,&#39;PRCA01&#39;)

AUC_comp&lt;-NULL
for(i in 1:length(pheno)){
  GWIZ_AUC_i&lt;-NULL
    GWIZ_AUC_i&lt;-read.table(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/GWIZ/&#39;,gwas[i],&#39;/res_&#39;,gwas[i],&#39;.log&#39;), sep=&#39;&amp;&#39;, header=F)$V1
    GWIZ_AUC_i_auc&lt;-as.numeric(gsub(&#39;AUC = &#39;,&#39;&#39;,GWIZ_AUC_i[grepl(&#39;AUC = &#39;, GWIZ_AUC_i)]))
    
  pheno_res_pTclump&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))

  Obs_AUC&lt;-pheno_res_pTclump$AUC[grepl(&#39;pT.clump&#39;, pheno_res_pTclump$Predictor) &amp; grepl(&#39;1e.06&#39;, pheno_res_pTclump$Predictor)]
  
  AUC_comp&lt;-rbind(AUC_comp, data.frame(Phenotype=pheno[i],
                                       Obs_AUC=Obs_AUC,
                                       GWIZ_AUC=GWIZ_AUC_i_auc))
}

write.csv(AUC_comp, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/GWIZ_AUC.csv&#39;, row.names=F, quote=F)

# GWIZ is not performing well, except for T2D, CAD, and Depression. Again, I think this might be due to improper preparation of GWAS summary statistics. GWIZ is also running quite slow, and would need to be run GW to get accurate estimates for polygenic outcomes. I don&#39;t think this is a viable option. This method is also not suitable for continous outcomes.</code></pre>
</details>
<details>
<p><summary>Show results</summary></p>
<table>
<caption>Estimation of PRS AUC using GWIZ</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">Obs_AUC</th>
<th align="right">GWIZ_AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Depression</td>
<td align="right">0.5300788</td>
<td align="right">0.5085728</td>
</tr>
<tr class="even">
<td align="left">T2D</td>
<td align="right">0.6011381</td>
<td align="right">0.5715527</td>
</tr>
<tr class="odd">
<td align="left">CAD</td>
<td align="right">0.5598304</td>
<td align="right">0.5610959</td>
</tr>
<tr class="even">
<td align="left">IBD</td>
<td align="right">0.6132864</td>
<td align="right">0.7349240</td>
</tr>
<tr class="odd">
<td align="left">MultiScler</td>
<td align="right">0.6577221</td>
<td align="right">0.5523625</td>
</tr>
<tr class="even">
<td align="left">RheuArth</td>
<td align="right">0.6024688</td>
<td align="right">0.7521208</td>
</tr>
<tr class="odd">
<td align="left">Breast_Cancer</td>
<td align="right">0.6258635</td>
<td align="right">0.7565655</td>
</tr>
<tr class="even">
<td align="left">Prostate_Cancer</td>
<td align="right">0.6631618</td>
<td align="right">0.8004651</td>
</tr>
</tbody>
</table>
</details>
<hr />
</div>
</div>
<div id="lassosum" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Lassosum</h3>
<p>Lassosum has a pseudovalidate option which estimates the correlation between predicted and observed values across shrinkage parameters. Although, its ability to estimate the best shrinakge parameter, the estimated correlation for the top shrinkage parameter appears similar to the observed correlation. We can convert the correlation into an R2 for continuous outcomes, and possible into an AUC, but converting to observed R2 into a liability R2, and then into an AUC. This would also be more suitable to predicting R2/AUC for the more modern shrinkage PRS methods, since lassosum is one.</p>
<hr />
<div id="pseudovalidate" class="section level4">
<h4><span class="header-section-number">2.1.3.1</span> Pseudovalidate</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

# Create directory
mkdir /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum

# Create file listing GWAS that haven&#39;t been processed.
&gt; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo.txt
for gwas in $(echo DEPR06 COLL01 BODY04 HEIG03 DIAB05 COAD01 CROH01 SCLE03 RHEU02 BRCA01 PRCA01);do
if [ ! -f /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/${gwas}/lassosum_pseudo_${gwas}.pseudoval.txt ]; then
echo $gwas &gt;&gt; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo.txt
fi
done

# Create shell script to run using sbatch
cat &gt; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/sbatch.sh &lt;&lt; &#39;EOF&#39;
#!/bin/sh

#SBATCH -p shared,brc
#SBATCH --mem=20G
#SBATCH -n 2
#SBATCH -J lassosum

. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

gwas=$(sed &quot;${SLURM_ARRAY_TASK_ID}q;d&quot; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo.txt)
echo ${gwas}

/users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/lassosum_pseudovalidate/lassosum_pseudovalidate.R \
    --ref_plink_gw ${Geno_1KG_dir}/1KGPhase3.w_hm3.GW \
    --ref_keep ${Geno_1KG_dir}/keep_files/EUR_samples.keep \
  --sumstats ${gwas_rep_qcd}/${gwas}.cleaned.gz \
    --output /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/${gwas}/lassosum_pseudo_${gwas} \
    --n_cores 2
EOF

sbatch --array 1-$(wc -l /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo.txt | cut -d&#39; &#39; -f1)%5 /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/sbatch.sh

. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

##########
# Run excluding MHC
##########

# Create directory
mkdir /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum

# Create file listing GWAS that haven&#39;t been processed.
&gt; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo_noMHC.txt
for gwas in $(echo DEPR06 COLL01 BODY04 HEIG03 DIAB05 COAD01 CROH01 SCLE03 RHEU02 BRCA01 PRCA01);do
if [ ! -f /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/${gwas}_noMHC/lassosum_pseudo_${gwas}.pseudoval.txt ]; then
echo $gwas &gt;&gt; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo_noMHC.txt
fi
done

# Create shell script to run using sbatch
cat &gt; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/sbatch_noMHC.sh &lt;&lt; &#39;EOF&#39;
#!/bin/sh

#SBATCH -p shared,brc
#SBATCH --mem=20G
#SBATCH -n 2
#SBATCH -J lassosum

. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

gwas=$(sed &quot;${SLURM_ARRAY_TASK_ID}q;d&quot; /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo_noMHC.txt)
echo ${gwas}

/users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/lassosum_pseudovalidate/lassosum_pseudovalidate.R \
    --ref_plink_gw ${Geno_1KG_dir}/1KGPhase3.w_hm3.GW \
    --ref_keep ${Geno_1KG_dir}/keep_files/EUR_samples.keep \
  --sumstats ${gwas_rep_qcd}/${gwas}.cleaned.gz \
  --prune_mhc T \
    --output /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/${gwas}_noMHC/lassosum_pseudo_${gwas} \
    --plink /users/k1806347/brc_scratch/Software/plink1.9.sh \
    --n_cores 2
EOF

sbatch --array 1-$(wc -l /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/todo_noMHC.txt | cut -d&#39; &#39; -f1)%5 /scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/sbatch_noMHC.sh

opt$ref_plink_gw&lt;-&#39;/users/k1806347/brc_scratch/Data/1KG/Phase3/1KGPhase3.w_hm3.GW&#39;
opt$ref_keep&lt;-&#39;/users/k1806347/brc_scratch/Data/1KG/Phase3/keep_files/EUR_samples.keep&#39;
opt$sumstats&lt;-&#39;/users/k1806347/brc_scratch/Data/GWAS_sumstats/prs_comparison/cleaned/SCLE03.cleaned.gz&#39;
opt$exclude_mhc&lt;-T
opt$output&lt;-&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/SCLE03_noMHC/lassosum_pseudo_SCLE03&#39;
opt$n_cores&lt;-2
opt$plink&lt;-&#39;/users/k1806347/brc_scratch/Software/plink1.9.sh&#39;
</code></pre>
</details>
<hr />
</div>
<div id="compare-observed-and-estimate-auc-1" class="section level4">
<h4><span class="header-section-number">2.1.3.2</span> Compare observed and estimate AUC</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

#####
# Binary outcomes
#####
pheno=c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
gwas=c(&#39;DEPR06&#39;,&#39;DIAB05&#39;,&#39;COAD01&#39;,&#39;CROH01&#39;,&#39;SCLE03&#39;,&#39;RHEU02&#39;,&#39;BRCA01&#39;,&#39;PRCA01&#39;)
samp_prev=c(0.28,0.168,0.33,0.285,0.36,0.246,0.537,0.564)

AUC_comp&lt;-NULL
for(i in 1:length(pheno)){
  lasso_AUC_i&lt;-NULL
  lasso_AUC_i&lt;-read.table(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/&#39;,gwas[i],&#39;_noMHC/lassosum_pseudo_&#39;,gwas[i],&#39;.log&#39;), sep=&#39;&amp;&#39;, header=F)$V1
  lasso_AUC_i_r&lt;-as.numeric(gsub(&#39;value = &#39;,&#39;&#39;,lasso_AUC_i[grepl(&#39;value = &#39;, lasso_AUC_i)]))
  
  n_case&lt;-samp_prev[i]
  n_con&lt;-1-samp_prev[i]
  
  a&lt;-(n_case+n_con)^2/(n_case*n_con)
  
  lasso_AUC_i_d&lt;-sqrt(a)*lasso_AUC_i_r/sqrt(1-lasso_AUC_i_r^2)
  lasso_AUC_i_auc &lt;- pnorm(lasso_AUC_i_d/sqrt(2), 0, 1)
  
  pheno_res&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))

  Obs_pT_clump_AUC&lt;-pheno_res$AUC[grepl(&#39;_1$&#39;, pheno_res$Predictor)]
  Obs_PRScs_AUC&lt;-pheno_res$AUC[grepl(&#39;_phiauto&#39;, pheno_res$Predictor)]

  AUC_comp&lt;-rbind(AUC_comp, data.frame(Phenotype=pheno[i],
                                       Obs_pTclump_pt1_AUC=Obs_pT_clump_AUC,
                                       Obs_PRScs_AUC=Obs_PRScs_AUC,
                                       Est_lasso_AUC=lasso_AUC_i_auc))
}

write.csv(AUC_comp, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/lassosum_AUC.csv&#39;, row.names=F, quote=F)

#####
# Continuous outcomes
#####
pheno=c(&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;)
gwas=c(&#39;COLL01&#39;,&#39;BODY04&#39;,&#39;HEIG03&#39;)

R2_comp&lt;-NULL
for(i in 1:length(pheno)){
    lasso_R2_i&lt;-NULL
    lasso_R2_i&lt;-read.table(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/&#39;,gwas[i],&#39;_noMHC/lassosum_pseudo_&#39;,gwas[i],&#39;.log&#39;), sep=&#39;&amp;&#39;, header=F)$V1
    lasso_R2_i_r&lt;-as.numeric(gsub(&#39;value = &#39;,&#39;&#39;,lasso_R2_i[grepl(&#39;value = &#39;, lasso_R2_i)]))
    
    lasso_R2_i_r2&lt;-lasso_R2_i_r^2

    pheno_res&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))
  
    Obs_pT_clump_R2&lt;-pheno_res$Obs_R2[grepl(&#39;_1$&#39;, pheno_res$Predictor)]
    Obs_PRScs_R2&lt;-pheno_res$Obs_R2[grepl(&#39;_phiauto&#39;, pheno_res$Predictor)]
    
    R2_comp&lt;-rbind(R2_comp, data.frame(Phenotype=pheno[i],
                                         Obs_pTclump_pT1_R2=Obs_pT_clump_R2,
                                         Obs_PRScs_R2=Obs_PRScs_R2,
                                         lasso_R2=lasso_R2_i_r2))
}

write.csv(R2_comp, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/lassosum_R2.csv&#39;, row.names=F, quote=F)

# This is looking really promising. Lassosum has some trouble with IBD, MultiScler and RheuArth, but otherwise is very accurate. These outcomes are all rarer and autoimmune disorders. I have run without mhc region but results still poor for these outcomes. Discordant findings may be due to phenotype heterogeneity between GWAS and UKB.</code></pre>
</details>
<details>
<p><summary>Show results</summary></p>
<table>
<caption>Estimation of PRS AUC using lassosum</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">Obs_pTclump_pt1_AUC</th>
<th align="right">Obs_PRScs_AUC</th>
<th align="right">Est_lasso_AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Depression</td>
<td align="right">0.5666770</td>
<td align="right">0.5783668</td>
<td align="right">0.5808219</td>
</tr>
<tr class="even">
<td align="left">T2D</td>
<td align="right">0.5920100</td>
<td align="right">0.6401091</td>
<td align="right">0.6513627</td>
</tr>
<tr class="odd">
<td align="left">CAD</td>
<td align="right">0.5759404</td>
<td align="right">0.6033915</td>
<td align="right">0.6238896</td>
</tr>
<tr class="even">
<td align="left">IBD</td>
<td align="right">0.5816432</td>
<td align="right">0.6293128</td>
<td align="right">0.8545295</td>
</tr>
<tr class="odd">
<td align="left">MultiScler</td>
<td align="right">0.6150556</td>
<td align="right">0.6821434</td>
<td align="right">0.7849519</td>
</tr>
<tr class="even">
<td align="left">RheuArth</td>
<td align="right">0.5823766</td>
<td align="right">0.6501171</td>
<td align="right">0.5000000</td>
</tr>
<tr class="odd">
<td align="left">Breast_Cancer</td>
<td align="right">0.6016412</td>
<td align="right">0.6583790</td>
<td align="right">0.6975121</td>
</tr>
<tr class="even">
<td align="left">Prostate_Cancer</td>
<td align="right">0.6316156</td>
<td align="right">0.6975937</td>
<td align="right">0.7207124</td>
</tr>
</tbody>
</table>
<table>
<caption>Estimation of PRS R2 using lassosum</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">Obs_pTclump_pT1_R2</th>
<th align="right">Obs_PRScs_R2</th>
<th align="right">lasso_R2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intelligence</td>
<td align="right">0.0062422</td>
<td align="right">0.0095257</td>
<td align="right">0.0229240</td>
</tr>
<tr class="even">
<td align="left">BMI</td>
<td align="right">0.0576659</td>
<td align="right">0.0796553</td>
<td align="right">0.0742400</td>
</tr>
<tr class="odd">
<td align="left">Height</td>
<td align="right">0.0634522</td>
<td align="right">0.1195499</td>
<td align="right">0.1563238</td>
</tr>
</tbody>
</table>
</details>
<hr />
<p>Based on these results, none of these approaches are brilliant. The AVENGEME/LDSC method is limited by the accuracy of the LDSC heritability estimate, and the ‘medium’ polygencity assumed by using a pi0 of 0.94. The lassosum approach does fairly well, but is not good for the autoimmune disorders. I think we should use the lassosum estimate unless it is R2=0 or AUC=0.5.</p>
<hr />
</div>
</div>
<div id="compare-absolute-estimatesvalues-when-using-lassosum-aucr2" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Compare absolute estimates/values when using lassosum AUC/R2</h3>
<hr />
<div id="binary" class="section level4">
<h4><span class="header-section-number">2.1.4.1</span> Binary</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Target_scoring.config&#39;)

pheno=c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
gwas=c(&#39;DEPR06&#39;,&#39;DIAB05&#39;,&#39;COAD01&#39;,&#39;CROH01&#39;,&#39;SCLE03&#39;,&#39;RHEU02&#39;,&#39;BRCA01&#39;,&#39;PRCA01&#39;)

n_quant&lt;-20

files&lt;-data.frame(pheno,gwas)

# Create function
ccprobs.f &lt;- function(PRS_auc=0.641, prev=0.7463, n_quantile=20){
  
    # Convert AUC into cohen&#39;s d
    d &lt;- sqrt(2)*qnorm(PRS_auc)
    
    # Set mean difference between cases and control polygenic scores
    mu_case &lt;- d
    mu_control &lt;- 0
    
    # Estimate mean and variance of polygenic scores across case and control
    varPRS &lt;- prev*(1+(d^2) - (d*prev)^2) + (1-prev)*(1 - (d*prev)^2)
    E_PRS &lt;- d*prev
    
    # Estimate polygenic score quantiles
    by_quant&lt;-1/n_quantile
    p_quant &lt;- seq(by_quant, 1-by_quant, by=by_quant)
    quant_vals_PRS &lt;- rep(0, length(p_quant))
    quant_f_solve &lt;- function(x, prev, d, pq){prev*pnorm(x-d) + (1-prev)*pnorm(x) - pq}
    for(i in 1:length(p_quant)){
        quant_vals_PRS[i] &lt;- unlist(uniroot(quant_f_solve, prev=prev, d=d, pq= p_quant[i], interval=c(-2.5, 2.5), extendInt = &quot;yes&quot;, tol=6e-12)$root)
    }
    
    # Create a table for output
    ul_qv_PRS &lt;- matrix(0, ncol=2, nrow=n_quantile)
    ul_qv_PRS[1,1] &lt;- -Inf
    ul_qv_PRS[2:n_quantile,1] &lt;- quant_vals_PRS
    ul_qv_PRS[1:(n_quantile-1),2] &lt;- quant_vals_PRS
    ul_qv_PRS[n_quantile,2] &lt;- Inf
    
    ul_qv_PRS&lt;-cbind(ul_qv_PRS, (ul_qv_PRS[,1:2]-E_PRS)/sqrt(varPRS))
    
    # Estimate case control proportion for each quantile
    prob_quantile_case &lt;- pnorm(ul_qv_PRS[,2], mean = mu_case) - pnorm(ul_qv_PRS[,1], mean = mu_case)
    prob_quantile_control &lt;- pnorm(ul_qv_PRS[,2], mean = mu_control) - pnorm(ul_qv_PRS[,1], mean = mu_control)
    p_case_quantile &lt;- (prob_quantile_case*prev)/by_quant
    p_cont_quantile &lt;- (prob_quantile_control*(1-prev))/by_quant
    
    # Estimate OR comparing each quantile to bottom quantile
    OR &lt;- p_case_quantile/p_cont_quantile
    OR &lt;- OR/OR[1]
    
    # Return output
    out &lt;- cbind(ul_qv_PRS[,3:4],p_cont_quantile, p_case_quantile, OR)
    row.names(out) &lt;- 1:n_quantile
    colnames(out) &lt;- c(&quot;q_min&quot;, &quot;q_max&quot;,&quot;p_control&quot;, &quot;p_case&quot;, &quot;OR&quot;)
    
    data.frame(out)
}

# Read in lassosum AUC
lassosum_AUC&lt;-read.csv(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/lassosum_AUC.csv&#39;)

# Run analysis for each phenotype
res_all&lt;-NULL
cor_res&lt;-NULL
plots_all&lt;-list()

for(i in 1:dim(files)[1]){
  # Read in pheno and prs data, and merge
  pheno_i&lt;-fread(paste0(UKBB_output,&#39;/Phenotype/PRS_comp_subset/UKBB.&#39;,files$pheno[i],&#39;.txt&#39;))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  prs_i&lt;-fread(paste0(UKBB_output,&#39;/PRS_for_comparison/1KG_ref/PRScs/&#39;,files$gwas[i],&#39;/UKBB.subset.w_hm3.&#39;,files$gwas[i],&#39;.PRScs_profiles&#39;))
  prs_i&lt;-prs_i[,c(&#39;FID&#39;,&#39;IID&#39;,paste0(files$gwas[i], &#39;_phiauto&#39;)), with=F]
  names(prs_i)[3]&lt;-&#39;prs&#39;

  pheno_prs&lt;-merge(pheno_i, prs_i, by=c(&#39;FID&#39;,&#39;IID&#39;))
  
  # Read in AUC for PRS
  assoc&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,files$pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,files$gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))
  prs_auc&lt;-assoc[grepl(&#39;phiauto&#39;, assoc$Predictor),]$AUC
  
  # Assign individuals to observed PRS quantiles
  obs_quant&lt;-quantile(pheno_prs$prs, prob = seq(0, 1, length = n_quant+1))
  pheno_prs$obs_quant&lt;-as.numeric(cut( pheno_prs$prs, obs_quant, include.lowest = T))

  # Calculate proportion of each quantile that are cases
  obs_cc&lt;-NULL
  for(k in 1:n_quant){
    obs_cc&lt;-rbind(obs_cc, data.frame(Phenotype=files$pheno[i],
                                     Type=&#39;Observed&#39;,
                                     Quantile=k,
                                     q_min=obs_quant[k],
                                     q_max=obs_quant[k+1],
                                     p_control=1-mean(pheno_prs$pheno[pheno_prs$obs_quant == k]),
                                     p_case=mean(pheno_prs$pheno[pheno_prs$obs_quant == k])))
  }
  
  # Assign individuals to estimated PRS quantiles
  est_cc&lt;-ccprobs.f(PRS_auc = prs_auc, prev=mean(pheno_prs$pheno), n_quantile = n_quant)
  est_cc$OR&lt;-NULL
  est_cc&lt;-data.frame(Phenotype=files$pheno[i],Type=&#39;Estimated&#39;,Quantile=1:n_quant, est_cc)
  est_quant&lt;-sort(unique(c(est_cc$q_min, est_cc$q_max)))
  pheno_prs$est_quant&lt;-as.numeric(cut( pheno_prs$prs, est_quant, include.lowest = T))
  
  tmp&lt;-cor.test(obs_cc$p_case, est_cc$p_case)

  # Assign individuals to estimated PRS quantiles
  lasso_est_cc&lt;-ccprobs.f(PRS_auc = lassosum_AUC$Est_lasso_AUC[lassosum_AUC$Phenotype == files$pheno[i]], prev=mean(pheno_prs$pheno), n_quantile = n_quant)
  lasso_est_cc$OR&lt;-NULL
  lasso_est_cc&lt;-data.frame(Phenotype=files$pheno[i],Type=&quot;Estimated (lasso)&quot;,Quantile=1:n_quant, lasso_est_cc)
  lasso_est_quant&lt;-sort(unique(c(lasso_est_cc$q_min, lasso_est_cc$q_max)))
  pheno_prs$lasso_est_quant&lt;-as.numeric(cut( pheno_prs$prs, lasso_est_quant, include.lowest = T))

  tmp2&lt;-cor.test(obs_cc$p_case, lasso_est_cc$p_case)
  
  # Estimate correlation between observed and expected
  cor_res&lt;-rbind(cor_res,data.frame(Phenotype=files$pheno[i],
                                    Cor=tmp$estimate,
                                    Low95CI=tmp$conf.int[1],
                                    High95CI=tmp$conf.int[2],
                                    Cor_lasso=tmp2$estimate,
                                    Low95CI_lasso=tmp2$conf.int[1],
                                    High95CI_lasso=tmp2$conf.int[2],
                                    N=length(pheno_prs$pheno),
                                    Ncas=sum(pheno_prs$pheno == 1),
                                    Ncon=sum(pheno_prs$pheno == 0)))

  quant_comp&lt;-do.call(rbind,list(obs_cc, est_cc, lasso_est_cc))
  
  res_all&lt;-rbind(res_all, quant_comp)
  
  library(ggplot2)
  library(cowplot)
  
  plots_all[[i]]&lt;-ggplot(quant_comp, aes(x=Quantile, y=p_case, colour=Type)) +
                    geom_point(alpha=0.8) +
                    geom_line(alpha=0.8) +
                    labs(x=&#39;PRS quantile&#39;, y=paste0(&#39;Proportion with &#39;,files$pheno[i]), title=files$pheno[i], colour=&#39;Method&#39;) +
                    theme_cowplot(12)
}

png(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/PropCC_Comp_lasso.png&#39;), units=&#39;px&#39;, res=300, width=2000, height=3000)
  plot_grid(plotlist=plots_all, ncol = 2)
dev.off()

write.csv(cor_res, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/PropCC_Comp_lasso.csv&#39;, row.names=F, quote=F)</code></pre>
</details>
<details>
<p><summary>Show results</summary> <img src="Images/Validating_ImputeMe_Ideas/PropCC_Comp_lasso.png" alt="Observed-Estimated Proportion of Cases within Polygenic Score Quantiles" /></p>
<table>
<caption>Correlation between observed and estimated proportion of cases within polygenic score quantiles</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">Cor</th>
<th align="right">Low95CI</th>
<th align="right">High95CI</th>
<th align="right">Cor_lasso</th>
<th align="right">Low95CI_lasso</th>
<th align="right">High95CI_lasso</th>
<th align="right">N</th>
<th align="right">Ncas</th>
<th align="right">Ncon</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Depression</td>
<td align="right">0.9937111</td>
<td align="right">0.9838077</td>
<td align="right">0.9975649</td>
<td align="right">0.9937042</td>
<td align="right">0.9837901</td>
<td align="right">0.9975622</td>
<td align="right">49999</td>
<td align="right">24999</td>
<td align="right">25000</td>
</tr>
<tr class="even">
<td align="left">T2D</td>
<td align="right">0.9954595</td>
<td align="right">0.9882933</td>
<td align="right">0.9982428</td>
<td align="right">0.9950837</td>
<td align="right">0.9873281</td>
<td align="right">0.9980972</td>
<td align="right">49999</td>
<td align="right">14888</td>
<td align="right">35111</td>
</tr>
<tr class="odd">
<td align="left">CAD</td>
<td align="right">0.9958911</td>
<td align="right">0.9894026</td>
<td align="right">0.9984101</td>
<td align="right">0.9960251</td>
<td align="right">0.9897470</td>
<td align="right">0.9984620</td>
<td align="right">49999</td>
<td align="right">25000</td>
<td align="right">24999</td>
</tr>
<tr class="even">
<td align="left">IBD</td>
<td align="right">0.9809581</td>
<td align="right">0.9514614</td>
<td align="right">0.9925978</td>
<td align="right">0.9244917</td>
<td align="right">0.8156651</td>
<td align="right">0.9701270</td>
<td align="right">49999</td>
<td align="right">3461</td>
<td align="right">46538</td>
</tr>
<tr class="odd">
<td align="left">MultiScler</td>
<td align="right">0.9733777</td>
<td align="right">0.9325383</td>
<td align="right">0.9896268</td>
<td align="right">0.9195593</td>
<td align="right">0.8043465</td>
<td align="right">0.9681262</td>
<td align="right">49999</td>
<td align="right">1137</td>
<td align="right">48862</td>
</tr>
<tr class="even">
<td align="left">RheuArth</td>
<td align="right">0.9844922</td>
<td align="right">0.9603604</td>
<td align="right">0.9939782</td>
<td align="right">0.0020514</td>
<td align="right">-0.4408696</td>
<td align="right">0.4441690</td>
<td align="right">49999</td>
<td align="right">3408</td>
<td align="right">46591</td>
</tr>
<tr class="odd">
<td align="left">Breast_Cancer</td>
<td align="right">0.9954801</td>
<td align="right">0.9883461</td>
<td align="right">0.9982508</td>
<td align="right">0.9917426</td>
<td align="right">0.9787724</td>
<td align="right">0.9968007</td>
<td align="right">49999</td>
<td align="right">8512</td>
<td align="right">41487</td>
</tr>
<tr class="even">
<td align="left">Prostate_Cancer</td>
<td align="right">0.9969747</td>
<td align="right">0.9921906</td>
<td align="right">0.9988298</td>
<td align="right">0.9951459</td>
<td align="right">0.9874879</td>
<td align="right">0.9981213</td>
<td align="right">50000</td>
<td align="right">2927</td>
<td align="right">47073</td>
</tr>
</tbody>
</table>
<p>Median Cor. = 0.9927234; Mean Cor. = 0.8522255; Min. Cor. = 0.002051437; Max. Cor. = 0.9960251</p>
<p>Excluding RheuArth: Median Cor. = 0.9937042; Mean Cor. = 0.9736789; Min. Cor. = 0.9195593; Max. Cor. = 0.9960251</p>
</details>
<hr />
</div>
<div id="continuous" class="section level4">
<h4><span class="header-section-number">2.1.4.2</span> Continuous</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)
library(e1071)

source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Target_scoring.config&#39;)

gwas&lt;-c(&#39;COLL01&#39;,&#39;HEIG03&#39;,&#39;BODY04&#39;)
pheno&lt;-c(&#39;Intelligence&#39;,&#39;Height&#39;,&#39;BMI&#39;)

n_quant&lt;-20

files&lt;-data.frame(pheno,gwas)

# Create function
mean_sd_quant.f &lt;- function(PRS_R2=0.641, Outcome_mean=1, Outcome_sd=1, n_quantile=20){
  ### PRS quantiles with a continuous phenotype (Y)
  library(tmvtnorm)
  ###
  E_PRS = 0
  SD_PRS = sqrt(1)
  E_phenotype = Outcome_mean
  SD_phenotype = Outcome_sd 

  by_quant&lt;-1/(n_quantile)
  PRS_quantile_bounds &lt;- qnorm(p=seq(0, 1, by=by_quant), mean= E_PRS, sd= SD_PRS)
  lower_PRS_vec &lt;- PRS_quantile_bounds[1:n_quantile]
  upper_PRS_vec &lt;- PRS_quantile_bounds[2:(n_quantile+1)]
  
  mean_vec &lt;- c(E_phenotype, E_PRS)
  sigma_mat &lt;- matrix(sqrt(PRS_R2)*SD_phenotype*SD_PRS, nrow=2, ncol=2)
  sigma_mat[1,1] &lt;- SD_phenotype^2
  sigma_mat[2,2] &lt;- SD_PRS^2
  
  ### mean of phenotype within the truncated PRS distribution
  out_mean_Y &lt;- rep(0, 20)
  ### SD of phenotype within the truncated PRS distribution
  out_SD_Y &lt;- rep(0, 20)
  ### cov of Y and PRS given truncation on PRS
  out_cov_Y_PRS &lt;- rep(0, 20)
  ### SD of PRS given truncation on PRS
  out_SD_PRS &lt;- rep(0, 20)
  ### mean PRS given truncation on PRS
  out_mean_PRS &lt;- rep(0, 20)
  
  for(i in 1:n_quantile){
    distribution_i &lt;- mtmvnorm(mean = mean_vec,
        sigma = sigma_mat,
        lower = c(-Inf, lower_PRS_vec[i]),
        upper = c(Inf, upper_PRS_vec[i]),
        doComputeVariance=TRUE,
        pmvnorm.algorithm=GenzBretz())
        out_mean_Y[i] &lt;- distribution_i$tmean[1]
        out_mean_PRS[i] &lt;- distribution_i$tmean[2]
        out_SD_Y[i] &lt;- sqrt(distribution_i$tvar[1,1])
        out_SD_PRS[i] &lt;- sqrt(distribution_i$tvar[2,2])
        out_cov_Y_PRS[i] &lt;- distribution_i$tvar[1,2]
  }
  
  out&lt;-data.frame(q=1:n_quantile,
             q_min=lower_PRS_vec,
             q_max=upper_PRS_vec,
             x_mean=out_mean_Y,
             x_sd=out_SD_Y)
  
  return(out)

  out_mean_Y
  out_SD_Y
  
  out_mean_PRS
  out_SD_PRS
  out_cov_Y_PRS
}

# Read in lassosum R2
lassosum_R2&lt;-read.csv(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Estimated_AUC_R2/lassosum/lassosum_R2.csv&#39;)

# Run analysis for each phenotype
res_all&lt;-NULL
plots_all&lt;-list()
cor_res&lt;-NULL

for(i in 1:dim(files)[1]){
  # Read in pheno and prs data, and merge
  pheno_i&lt;-fread(paste0(UKBB_output,&#39;/Phenotype/PRS_comp_subset/UKBB.&#39;,files$pheno[i],&#39;.txt&#39;))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  prs_i&lt;-fread(paste0(UKBB_output,&#39;/PRS_for_comparison/1KG_ref/PRScs/&#39;,files$gwas[i],&#39;/UKBB.subset.w_hm3.&#39;,files$gwas[i],&#39;.PRScs_profiles&#39;))
  prs_i&lt;-prs_i[,c(&#39;FID&#39;,&#39;IID&#39;,paste0(files$gwas[i], &#39;_phiauto&#39;)), with=F]
  names(prs_i)[3]&lt;-&#39;prs&#39;

  pheno_prs&lt;-merge(pheno_i, prs_i, by=c(&#39;FID&#39;,&#39;IID&#39;))
  
  # Read in AUC for PRS
  assoc&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/&#39;,files$pheno[i],&#39;/Association_withPRSs/UKBB.w_hm3.&#39;,files$gwas[i],&#39;.EUR-PRSs.AllMethodComp.assoc.txt&#39;))
  prs_r2&lt;-assoc[grepl(&#39;phiauto&#39;, assoc$Predictor),]$Obs_R2
  
  # Assign individuals to observed PRS quantiles
  obs_quant&lt;-quantile(pheno_prs$prs, prob = seq(0, 1, length = n_quant+1))
  pheno_prs$obs_quant&lt;-as.numeric(cut( pheno_prs$prs, obs_quant, include.lowest = T))

  # Calculate mean and SD of each quantile that are cases
  obs_dist&lt;-NULL
  for(k in 1:n_quant){
    obs_dist&lt;-rbind(obs_dist, data.frame(Phenotype=files$pheno[i],
                                     Type=&#39;Observed&#39;,
                                     Quantile=k,
                                     q_min=obs_quant[k],
                                     q_max=obs_quant[k+1],
                                     x_mean=mean(pheno_prs$pheno[pheno_prs$obs_quant == k]),
                                     x_sd=sd(pheno_prs$pheno[pheno_prs$obs_quant == k])))
  }
  
  # Assign individuals to estimated PRS quantiles
  est_dist&lt;-mean_sd_quant.f(PRS_R2 = prs_r2, Outcome_mean=mean(pheno_prs$pheno), Outcome_sd=sd(pheno_prs$pheno), n_quantile = n_quant)
  est_dist$q&lt;-NULL
  est_dist&lt;-data.frame(Phenotype=files$pheno[i],Type=&#39;Estimated&#39;,Quantile=1:n_quant, est_dist)
  est_quant&lt;-sort(unique(c(est_dist$q_min, est_dist$q_max)))
  pheno_prs$est_quant&lt;-as.numeric(cut( pheno_prs$prs, est_quant, include.lowest = T))

  # Assign individuals to lassosum estimated PRS quantiles
  lasso_est_dist&lt;-mean_sd_quant.f(PRS_R2 = lassosum_R2$lasso_R2[lassosum_AUC$Phenotype == files$pheno[i]], Outcome_mean=mean(pheno_prs$pheno), Outcome_sd=sd(pheno_prs$pheno), n_quantile = n_quant)
  lasso_est_dist$q&lt;-NULL
  lasso_est_dist&lt;-data.frame(Phenotype=files$pheno[i],Type=&quot;Estimated (lassosum)&quot;,Quantile=1:n_quant, lasso_est_dist)
  lasso_est_quant&lt;-sort(unique(c(lasso_est_dist$q_min, lasso_est_dist$q_max)))
  pheno_prs$lasso_est_quant&lt;-as.numeric(cut( pheno_prs$prs, lasso_est_quant, include.lowest = T))
  
  quant_comp&lt;-do.call(rbind, list(obs_dist, est_dist, lasso_est_dist))
  
  tmp&lt;-cor.test(obs_dist$x_mean, est_dist$x_mean)
  tmp2&lt;-cor.test(obs_dist$x_mean, lasso_est_dist$x_mean)
  
  cor_res&lt;-rbind(cor_res,data.frame(Phenotype=files$pheno[i],
                                    Cor_mean=tmp$estimate,
                                    Cor_mean_Low95CI=tmp$conf.int[1],
                                    Cor_mean_High95CI=tmp$conf.int[2],
                                    Cor_mean_lasso=tmp2$estimate,
                                    Cor_mean_Low95CI_lasso=tmp2$conf.int[1],
                                    Cor_mean_High95CI_lasso=tmp2$conf.int[2],
                                    PercDiff_sd_mean=mean(abs(est_dist$x_sd-obs_dist$x_sd)/obs_dist$x_sd),
                                    PercDiff_sd_mean_lasso=mean(abs(lasso_est_dist$x_sd-obs_dist$x_sd)/obs_dist$x_sd),
                                    N=length(pheno_prs$pheno),
                                    Skewness=skewness(pheno_prs$pheno)))

  res_all&lt;-rbind(res_all, quant_comp)
  
  library(ggplot2)
  library(cowplot)
  
  plots_all[[i]]&lt;-ggplot(quant_comp, aes(x=Quantile, y=x_mean, colour=Type)) +
    geom_point(stat=&quot;identity&quot;, position=position_dodge(.9), alpha=0.8, shape=18) +
    geom_errorbar(aes(ymin=x_mean-x_sd, ymax=x_mean+x_sd), width=.2, position=position_dodge(.9), alpha=0.8) +
    labs(x=&#39;PRS quantile&#39;, y=paste0(pheno[i], &quot; mean (SD)&quot;), title=files$pheno[i], colour=&#39;Method&#39;) +
    theme_cowplot(12)
  
}

png(paste0(&#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/Mean_SD_Comp_lasso.png&#39;), units=&#39;px&#39;, res=300, width=1500, height=1750)
  plot_grid(plotlist=plots_all, ncol = 1)
dev.off()

write.csv(cor_res, &#39;/scratch/users/k1806347/Analyses/AbsoluteRisk/Measured_AUC_R2/Mean_SD_Comp_lasso.csv&#39;, row.names=F, quote=F)</code></pre>
</details>
<details>
<p><summary>Show results</summary> <img src="Images/Validating_ImputeMe_Ideas/Mean_SD_Comp_lasso.png" alt="Observed-Estimated Proportion of Cases within Polygenic Score Quantiles" /></p>
<table>
<caption>Correlation between observed and estimated mean and standar deviation of outcome within polygenic score quantiles</caption>
<thead>
<tr class="header">
<th align="left">Phenotype</th>
<th align="right">Cor_mean</th>
<th align="right">Cor_mean_Low95CI</th>
<th align="right">Cor_mean_High95CI</th>
<th align="right">Cor_mean_lasso</th>
<th align="right">Cor_mean_Low95CI_lasso</th>
<th align="right">Cor_mean_High95CI_lasso</th>
<th align="right">PercDiff_sd_mean</th>
<th align="right">PercDiff_sd_mean_lasso</th>
<th align="right">N</th>
<th align="right">Skewness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intelligence</td>
<td align="right">0.9681484</td>
<td align="right">0.9196140</td>
<td align="right">0.9875691</td>
<td align="right">0.9681484</td>
<td align="right">0.9196140</td>
<td align="right">0.9875691</td>
<td align="right">0.0139143</td>
<td align="right">0.0154372</td>
<td align="right">50000</td>
<td align="right">0.1440539</td>
</tr>
<tr class="even">
<td align="left">Height</td>
<td align="right">0.9973953</td>
<td align="right">0.9932740</td>
<td align="right">0.9989926</td>
<td align="right">0.9973953</td>
<td align="right">0.9932740</td>
<td align="right">0.9989926</td>
<td align="right">0.0156650</td>
<td align="right">0.0216927</td>
<td align="right">49999</td>
<td align="right">0.1168512</td>
</tr>
<tr class="odd">
<td align="left">BMI</td>
<td align="right">0.9983971</td>
<td align="right">0.9958577</td>
<td align="right">0.9993802</td>
<td align="right">0.9983971</td>
<td align="right">0.9958577</td>
<td align="right">0.9993802</td>
<td align="right">0.0574380</td>
<td align="right">0.0578926</td>
<td align="right">49999</td>
<td align="right">0.5915969</td>
</tr>
</tbody>
</table>
<p>Median Cor. of means = 0.9973953; Mean Cor. of means = 0.9879803; Min. Cor. of means = 0.9681484; Max. Cor. of means = 0.9983971</p>
<p>Median mean %diff of SD = 0.02169266; Mean mean %diff of SD = 0.03167416; Min. mean %diff of SD = 0.01543719; Max. mean %diff of SD = 0.05789262</p>
</details>
<hr />
</div>
</div>
</div>
<div id="validation-using-cross-validation" class="section level2">
<h2><span class="header-section-number">2.2</span> Validation using Cross-validation</h2>
<p>Use the same phenotypes as other GenoPred projects. To maximise power, lets use a 10-fold cross validation procedure, whereby we split the cases and controls for each outcome into 10 parts, then perform a GWAS within each fold. Then we create GWAS sumstats using 9 of 10 subsets, and calculate polygenic scores from the results for the remaining part of the sample. I have already written code for a similar process when playing with the BWAS project.</p>
<hr />
<div id="split-ukb-into-10-roughly-equal-parts" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Split UKB into 10 roughly equal parts</h3>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>system(&#39;mkdir -p /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS&#39;)

# Create keep file for each fold
library(data.table)
QC&lt;-fread(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/ukb18177_glanville_post_qc_id_list.UpdateIDs.fam&#39;)
names(QC)&lt;-c(&#39;FID&#39;,&#39;IID&#39;)

N &lt;- 10
set.seed(1)
subsets&lt;-split(QC, sample(1:N, nrow(QC), replace=T))

for(i in 1:length(subsets)){
  write.table(subsets[[i]], paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_&#39;,i,&#39;.keep&#39;), col.names=F, row.names=F, quote=F)
}</code></pre>
</details>
<hr />
</div>
<div id="prepare-covariates-file" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Prepare covariates file</h3>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>
library(data.table)

# Read in covariate data
geno_covar&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/ukb18177_glanville_covariates.UpdateIDs.txt&#39;)
sex&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Sex.pheno&#39;)
sex$Sex&lt;-sex$Sex+1

age&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Age.pheno&#39;)

# Dummy code batch and assessment_centre
batch_dum&lt;-data.table(FID=geno_covar$FID, IID=geno_covar$IID, model.matrix(~ as.factor(batch)-1, geno_covar)[,-1]+1)
names(batch_dum)&lt;-c(&#39;FID&#39;,&#39;IID&#39;,paste0(&#39;batch&#39;, 1:(dim(batch_dum)[2]-2)))
assessment_centre_dum&lt;-data.table(FID=geno_covar$FID, IID=geno_covar$IID, model.matrix(~ as.factor(assessment_centre)-1, geno_covar)[,-1]+1)
names(assessment_centre_dum)&lt;-c(&#39;FID&#39;,&#39;IID&#39;,paste0(&#39;centre&#39;, 1:(dim(assessment_centre_dum)[2]-2)))
geno_covar$batch&lt;-NULL
geno_covar$assessment_centre&lt;-NULL

covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(geno_covar, sex, age, batch_dum,assessment_centre_dum))

write.table(covs, &#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/ukb18177_glanville_covariates.UpdateIDs.Age.Sex.forPLINK.txt&#39;, quote=F, col.names=T, row.names=F)
</code></pre>
</details>
<p>Note. Running a regression with all covariates does not work due to multicolinearity. It also takes a very long time to run the GWAS. An alternative approach which seems to work well is regressing covariates out in advance, and then using the residuals in a chi-square test.</p>
<hr />
</div>
<div id="calculate-phenotype-residuals" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Calculate phenotype residuals</h3>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

####
# Prepare covariate data
####

# Read in covariate data
geno_covar&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/ukb18177_glanville_covariates.UpdateIDs.txt&#39;)
sex&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Sex.pheno&#39;)
age&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Age.pheno&#39;)

covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(geno_covar, sex, age))

# Convert catagorical variables into factors so they are dummy coded in the regression
covs$batch&lt;-factor(covs$batch)
covs$assessment_centre&lt;-factor(covs$assessment_centre)

# Read in list of individuals passing QC
QC&lt;-fread(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/ukb18177_glanville_post_qc_id_list.UpdateIDs.fam&#39;)
names(QC)&lt;-c(&#39;FID&#39;,&#39;IID&#39;)

# Merge all data
covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(covs, QC))

###
# Residualise each phenotype within each subset
###

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
pheno_file&lt;-c(&#39;ever_depressed_pheno_final.UpdateIDs.txt&#39;,&#39;UKBB_Fluid.intelligence.score.UpdateIDs.pheno&#39;,&#39;UKBB_BMI.score.UpdateIDs.pheno&#39;,&#39;UKBB_Height.score.UpdateIDs.pheno&#39;,&#39;t2d_only_111119.UpdateIDs.txt&#39;,&#39;cad_only_111119.UpdateIDs.txt&#39;,&#39;UKBB.IBD.txt&#39;,&#39;UKBB.MultiScler.txt&#39;,&#39;UKBB.RheuArth.txt&#39;,&#39;UKBB_Breast_Cancer.pheno&#39;,&#39;UKBB_Prostate_Cancer.pheno&#39;)

for(i in 1:length(pheno)){
  print(pheno[i])
  
  # Read in phenotype file
  pheno_i&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/&#39;,pheno[i],&#39;/&#39;,pheno_file[i]))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  
  # Determine whether binary
  if(length(unique(pheno_i$pheno[!is.na(pheno_i$pheno)])) == 2){
    family_i&lt;-&#39;binomial&#39;
  } else {
    family_i&lt;-&#39;gaussian&#39;
  }
  
  # Merge pheno and covar data
  pheno_i_covs&lt;-merge(pheno_i, covs, by=c(&#39;FID&#39;,&#39;IID&#39;), all=T)
  
  pheno_i_resid&lt;-NULL
  for(subset_i in 1:10){
    # Read in subset keep file
    subset_keep&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_&#39;,subset_i,&#39;.keep&#39;))
    
    # Subset pheno and covar data
    pheno_i_covs_subset_i&lt;-pheno_i_covs[(pheno_i_covs$FID %in% subset_keep$V1),]
    
    # Calculate residuals
    resid_tmp&lt;-as.numeric(scale(resid(glm(pheno ~ ., family=family_i, data=pheno_i_covs_subset_i[,-1:-2], na.action=&#39;na.exclude&#39;))))
    
    pheno_i_resid_subset_i&lt;-data.frame(FID=pheno_i_covs_subset_i$FID,
                              IID=pheno_i_covs_subset_i$IID,
                              pheno_resid=resid_tmp)
    
    pheno_i_resid&lt;-rbind(pheno_i_resid, pheno_i_resid_subset_i)
  }
  
  # Save phenotype file
  write.table(pheno_i_resid, paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/&#39;,pheno[i],&#39;/&#39;,pheno[i],&#39;_LOO_resid.txt&#39;), col.names=T, row.name=F, quote=F)
  
}</code></pre>
</details>
<hr />
</div>
<div id="run-gwas-within-each-subset-for-each-phenotype" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Run GWAS within each subset for each phenotype</h3>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>
pheno=$(echo Depression Intelligence BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer)

for subset in $(seq 1 10);do
  mkdir /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_${subset}
done

for i in $(seq 1 11); do
    pheno_i=$(echo ${pheno} | cut -f ${i} -d &#39; &#39;)
  for subset in $(seq 1 10);do
    for chr in $(seq 1 22); do
        sbatch -p brc,shared --mem 5G -t 00:10:00 /users/k1806347/brc_scratch/Software/plink1.9.sh \
          --bfile /users/k1806347/brc_scratch/Data/UKBB/Genotype/Harmonised/UKBB.w_hm3.QCd.AllSNP.chr${chr} \
          --pheno /users/k1806347/brc_scratch/Data/UKBB/Phenotype/${pheno_i}/${pheno_i}_LOO_resid.txt \
          --keep /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_${subset}.keep \
          --assoc \
          --maf 0.01 \
          --geno 0.05 \
          --allow-no-sex \
          --out /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_${subset}/UKBB.w_hm3.QCd.AllSNP.Subset_${subset}.${pheno_i}.chr${chr}
      done
      sleep 100
  done
done
</code></pre>
</details>
<hr />
</div>
<div id="meta-analyse-loo-results" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Meta-analyse LOO results</h3>
<hr />
<div id="prepare-sumstats-for-metal" class="section level4">
<h4><span class="header-section-number">2.2.5.1</span> Prepare sumstats for METAL</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code># Format results for use in METAL
library(data.table)

# Read in bim file
bim&lt;-NULL
for(chr in 1:22){
  bim_chr&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/Genotype/Harmonised/UKBB.w_hm3.QCd.AllSNP.chr&#39;,chr,&#39;.bim&#39;))
  bim&lt;-rbind(bim,bim_chr)
}

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)

for(pheno_i in pheno){
  for(subset_i in 1:10){
  gwas_i&lt;-NULL
    for(chr in 1:22){
          gwas_i_chr&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_&#39;,subset_i,&#39;/UKBB.w_hm3.QCd.AllSNP.Subset_&#39;,subset_i,&#39;.&#39;,pheno_i,&#39;.chr&#39;,chr,&#39;.qassoc&#39;))
          gwas_i_chr&lt;-merge(gwas_i_chr, bim, by.x=&#39;SNP&#39;, by.y=&#39;V2&#39;)
          gwas_i_chr&lt;-gwas_i_chr[,c(&#39;CHR&#39;,&#39;SNP&#39;,&#39;BP&#39;,&#39;V5&#39;,&#39;V6&#39;,&#39;BETA&#39;,&#39;SE&#39;,&#39;P&#39;,&#39;NMISS&#39;)]
          names(gwas_i_chr)&lt;-c(&#39;CHR&#39;,&#39;SNP&#39;,&#39;BP&#39;,&#39;A1&#39;,&#39;A2&#39;,&#39;BETA&#39;,&#39;SE&#39;,&#39;P&#39;,&#39;N&#39;)
          gwas_i_chr&lt;-gwas_i_chr[complete.cases(gwas_i_chr),]
      gwas_i&lt;-rbind(gwas_i, gwas_i_chr)
    }
    
    fwrite(gwas_i, paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_&#39;,subset_i,&#39;/UKBB.w_hm3.QCd.AllSNP.Subset_&#39;,subset_i,&#39;.&#39;,pheno_i,&#39;.GW.qassoc.clean&#39;), sep=&#39; &#39;, na=&#39;NA&#39;, quote=F)
  }
}</code></pre>
</details>
<hr />
</div>
<div id="run-meta-analysis-leaving-one-subset-out-each-time" class="section level4">
<h4><span class="header-section-number">2.2.5.2</span> Run meta-analysis leaving one subset out each time</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>
# Delete per chromosome files
# rm /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/*/*.chr*

# Run METAL using all subsets first
mkdir /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta

# Iteratively meta-analyse across all bar one subsets using METAL.
# for pheno in $(echo Depression Intelligence BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer); do
for pheno in $(echo BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer); do
for subset in $(seq 1 10);do

cat &gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_subset_template.sh &lt;&lt;EOF
MARKER SNP
ALLELE A1 A2
EFFECT BETA
PVALUE P
SCHEME STDERR
STDERR SE

PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_1/UKBB.w_hm3.QCd.AllSNP.Subset_1.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_2/UKBB.w_hm3.QCd.AllSNP.Subset_2.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_3/UKBB.w_hm3.QCd.AllSNP.Subset_3.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_4/UKBB.w_hm3.QCd.AllSNP.Subset_4.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_5/UKBB.w_hm3.QCd.AllSNP.Subset_5.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_6/UKBB.w_hm3.QCd.AllSNP.Subset_6.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_7/UKBB.w_hm3.QCd.AllSNP.Subset_7.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_8/UKBB.w_hm3.QCd.AllSNP.Subset_8.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_9/UKBB.w_hm3.QCd.AllSNP.Subset_9.${pheno}.GW.qassoc.clean
PROCESS /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_10/UKBB.w_hm3.QCd.AllSNP.Subset_10.${pheno}.GW.qassoc.clean

OUTFILE /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_${subset}_${pheno} .tbl
ANALYZE 

QUIT

EOF

# Delete line containing subset
sed &quot;/\\.Subset_${subset}\\./d&quot; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_subset_template.sh &gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_subset_${subset}_${pheno}.sh

sbatch -p brc,shared /users/k1806347/brc_scratch/Software/metal.sh /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_subset_${subset}_${pheno}.sh

done
sleep 60
done

# Delete the script files
rm /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/*sh
</code></pre>
</details>
<hr />
</div>
</div>
<div id="create-a-file-listing-the-sample-size-for-each-variant-and-all-phenotype" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Create a file listing the sample size for each variant and all phenotype</h3>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)

for(pheno_i in pheno){
  for(subset in as.character(1:10)){
    gwas&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_&#39;,subset,&#39;/UKBB.w_hm3.QCd.AllSNP.Subset_&#39;,subset,&#39;.&#39;,pheno_i,&#39;.GW.qassoc.clean&#39;))
    
    if(subset == 1){
        N_tab_all&lt;-gwas[,c(&#39;SNP&#39;,&#39;N&#39;),with=F]
    } else {
        N_tab_all&lt;-merge(N_tab_all, gwas[,c(&#39;SNP&#39;,&#39;N&#39;),with=F], by=&#39;SNP&#39;)
    }
    
    names(N_tab_all)[names(N_tab_all) == &#39;N&#39;]&lt;-paste0(&#39;Subset_&#39;,subset,&#39;_N&#39;)

  }
  
  write.table(N_tab_all, paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/N_table_&#39;,pheno_i,&#39;.txt&#39;), col.names=T, row.names=F, quote=F)
}</code></pre>
</details>
<hr />
</div>
<div id="polygenic-scoring" class="section level3">
<h3><span class="header-section-number">2.2.7</span> Polygenic scoring</h3>
<p>Now we calculate polygenic scores for these phenotypes in UK Biobank, using the independent summary statistics. Lets use the standard PRScs method (best) and pT+clump method (traditional).</p>
<hr />
<div id="format-the-loo-meta-analysis-results-for-polygenic-scoring" class="section level4">
<h4><span class="header-section-number">2.2.7.1</span> Format the LOO-meta-analysis results for polygenic scoring</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

source(&#39;/users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config&#39;)
pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)

# Read in bim file
bim&lt;-NULL
for(chr in 1:22){
  bim_chr&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/Genotype/Harmonised/UKBB.w_hm3.QCd.AllSNP.chr&#39;,chr,&#39;.bim&#39;))
  bim&lt;-rbind(bim,bim_chr)
}

for(pheno_i in pheno){
  for(subset_i in 1:10){
    gwas&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno_i,&#39;1.tbl&#39;))
    
    gwas&lt;-merge(gwas[,c(&#39;MarkerName&#39;,&#39;Allele1&#39;,&#39;Allele2&#39;,&#39;Effect&#39;,&#39;StdErr&#39;,&#39;P-value&#39;), with=F], bim[,c(&#39;V1&#39;,&#39;V2&#39;,&#39;V4&#39;),with=F], by.x=&#39;MarkerName&#39;, by.y=&#39;V2&#39;)
    names(gwas)&lt;-c(&#39;SNP&#39;,&#39;A1&#39;,&#39;A2&#39;,&#39;BETA&#39;,&#39;SE&#39;,&#39;P&#39;,&#39;CHR&#39;,&#39;ORIGBP&#39;)

    gwas$A1&lt;-toupper(gwas$A1)
    gwas$A2&lt;-toupper(gwas$A2)
    
    N_tab&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/N_table_&#39;,pheno_i,&#39;.txt&#39;))
    
    N_tab$excl_subset&lt;-rowSums(N_tab[,names(N_tab)[!grepl(paste0(&#39;SNP|Subset_&#39;,subset_i,&#39;_N&#39;), names(N_tab))],with=F])
  
    gwas&lt;-merge(gwas, N_tab[,c(&#39;SNP&#39;,&#39;excl_subset&#39;), with=F], by=&#39;SNP&#39;)
    
    names(gwas)[names(gwas) == &#39;excl_subset&#39;]&lt;-&#39;N&#39;
    gwas&lt;-gwas[,c(&#39;CHR&#39;,&#39;SNP&#39;,&#39;ORIGBP&#39;,&#39;A1&#39;,&#39;A2&#39;,&#39;BETA&#39;,&#39;SE&#39;,&#39;P&#39;,&#39;N&#39;), with=F]
    gwas&lt;-gwas[order(gwas$CHR, gwas$ORIGBP),]
  
    fwrite(gwas, paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno_i,&#39;.clean&#39;), sep=&#39; &#39;, na=&#39;NA&#39;, quote=F)
    
    # Run through GWAS sumstat QC script
    system(paste0(&#39;/users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/sumstat_cleaner/sumstat_cleaner.R --sumstats /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno_i,&#39;.clean --ref_plink_chr &#39;,Geno_1KG_dir,&#39;/1KGPhase3.w_hm3.chr --ref_freq_chr &#39;,Geno_1KG_dir,&#39;/freq_files/EUR/1KGPhase3.w_hm3.EUR.chr --output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno_i,&#39;.clean.QC&#39;))
  }
}</code></pre>
</details>
<hr />
</div>
<div id="ptclump" class="section level4">
<h4><span class="header-section-number">2.2.7.2</span> pT+clump</h4>
<hr />
<div id="run-ptclump" class="section level5">
<h5><span class="header-section-number">2.2.7.2.1</span> Run pT+clump</h5>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>for pheno in $(echo Depression Intelligence BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer); do
  for subset in $(seq 1 10);do

    sbatch -p brc,shared --mem 6G /users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/polygenic_score_file_creator/polygenic_score_file_creator.R \
      --ref_plink_chr /users/k1806347/brc_scratch/Data/1KG/Phase3/1KGPhase3.w_hm3.chr \
      --sumstats /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_${subset}_${pheno}.clean.QC.gz \
      --plink /users/k1806347/brc_scratch/Software/plink1.9.sh \
      --memory 5000 \
      --output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/pTclump/${pheno}/excl_Subset_${subset}/pTclump_excl_Subset_${subset}_${pheno} \
      --ref_pop_scale /users/k1806347/brc_scratch/Data/1KG/Phase3/super_pop_keep.list
done
done

# Calculate polygenic scores in each subset
for pheno in $(echo Depression Intelligence BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer); do
  for subset in $(seq 1 10);do
  
    mkdir -p /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/PRScs/${pheno}/Subset_${subset}
    
    sbatch --mem 10G -p brc,shared -J pT_clump /users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/Scaled_polygenic_scorer/Scaled_polygenic_scorer.R \
      --target_plink_chr /users/k1806347/brc_scratch/Data/UKBB/Genotype/Harmonised/UKBB.w_hm3.QCd.AllSNP.chr \
      --target_keep /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_${subset}.keep \
    --ref_score /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/pTclump/${pheno}/excl_Subset_${subset}/pTclump_excl_Subset_${subset}_${pheno} \
    --ref_scale /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/pTclump/${pheno}/excl_Subset_${subset}/pTclump_excl_Subset_${subset}_${pheno}.EUR.scale \
    --pheno_name ${pheno} \
    --ref_freq_chr /users/k1806347/brc_scratch/Data/1KG/Phase3/freq_files/EUR/1KGPhase3.w_hm3.EUR.chr \
    --plink /users/k1806347/brc_scratch/Software/plink1.9.sh \
    --output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/${pheno}/Subset_${subset}/Subset_${subset}_${pheno}_pTclump

  done
done
</code></pre>
</details>
<hr />
</div>
<div id="evaluate-predictive-utility-of-prs" class="section level5">
<h5><span class="header-section-number">2.2.7.2.2</span> Evaluate predictive utility of PRS</h5>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code># Combine the polygenic scores for each subset
library(data.table)

####
# Prepare covariate data
####

# Read in covariate data
geno_covar&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/ukb18177_glanville_covariates.UpdateIDs.txt&#39;)
sex&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Sex.pheno&#39;)
age&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Age.pheno&#39;)

covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(geno_covar, sex, age))

# Convert catagorical variables into factors so they are dummy coded in the regression
covs$batch&lt;-factor(covs$batch)
covs$assessment_centre&lt;-factor(covs$assessment_centre)

# Read in list of individuals passing QC
QC&lt;-fread(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/ukb18177_glanville_post_qc_id_list.UpdateIDs.fam&#39;)
names(QC)&lt;-c(&#39;FID&#39;,&#39;IID&#39;)

# Merge all data
covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(covs, QC))

###
# Read in pheno type and PRS data, regress covariates from PRS within subset, and the estimate R2 and AUC across subsets.
###

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
pheno_file&lt;-c(&#39;ever_depressed_pheno_final.UpdateIDs.txt&#39;,&#39;UKBB_Fluid.intelligence.score.UpdateIDs.pheno&#39;,&#39;UKBB_BMI.score.UpdateIDs.pheno&#39;,&#39;UKBB_Height.score.UpdateIDs.pheno&#39;,&#39;t2d_only_111119.UpdateIDs.txt&#39;,&#39;cad_only_111119.UpdateIDs.txt&#39;,&#39;UKBB.IBD.txt&#39;,&#39;UKBB.MultiScler.txt&#39;,&#39;UKBB.RheuArth.txt&#39;,&#39;UKBB_Breast_Cancer.pheno&#39;,&#39;UKBB_Prostate_Cancer.pheno&#39;)
prev=c(0.15,NA,NA,NA,0.05,0.03,0.013,0.00164,0.005,0.125,0.125)

for(i in 1:length(pheno_file)){
  print(pheno[i])
  
  # Read in phenotype file
  pheno_i&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/&#39;,pheno[i],&#39;/&#39;,pheno_file[i]))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  
  # Determine whether binary
  if(length(unique(pheno_i$pheno[!is.na(pheno_i$pheno)])) == 2){
    family_i&lt;-&#39;binomial&#39;
  } else {
    family_i&lt;-&#39;gaussian&#39;
  }
  
  # Merge pheno and covar data
  pheno_i_covs&lt;-merge(pheno_i, covs, by=c(&#39;FID&#39;,&#39;IID&#39;), all=T)

  pheno_res&lt;-NULL
  pheno_i_covs_prs&lt;-NULL
  for(subset_i in 1:10){
    print(subset_i)
    # Read in PRS for subset
    prs_var_subset&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;/Subset_&#39;,subset_i,&#39;/Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;_pTclump.profiles&#39;))
    
    names(prs_var_subset)&lt;-gsub(&#39;-&#39;,&#39;_&#39;,names(prs_var_subset))

    if(subset_i != 1){
      prs_var_subset&lt;-prs_var_subset[,(names(prs_var_subset) %in% names(pheno_i_covs_prs)), with=F]
    }
    
    # Identify pT columns
    pT&lt;-gsub(paste0(pheno[i],&#39;_&#39;),&#39;&#39;,names(prs_var_subset)[-1:-2])

    # Merge with pheno and covs
    pheno_i_covs_prs_subset&lt;-merge(pheno_i_covs, prs_var_subset, by=c(&#39;FID&#39;,&#39;IID&#39;))
    
    # Remove incomplete rows
    pheno_i_covs_prs_subset&lt;-pheno_i_covs_prs_subset[complete.cases(pheno_i_covs_prs_subset),]
    
    for(pT_i in pT){
    
    # Regress covariates out of the PRS
    pheno_i_covs_prs_subset[[paste0(pheno[i],&#39;_&#39;,pT_i)]]&lt;-as.numeric(scale(resid(glm(paste0(pheno[i],&#39;_&#39;,pT_i,&#39; ~ &#39;,paste(names(covs)[-1:-2],collapse=&#39;+&#39;)), family=&#39;gaussian&#39;, data=pheno_i_covs_prs_subset[,-1:-3]))))
    
    }
    
    pheno_i_covs_prs&lt;-rbind(pheno_i_covs_prs, pheno_i_covs_prs_subset)
  }
  
  # Run regression between outcome and PRS residuals
  for(pT_i in pT){

    model&lt;-glm(paste0(&#39;pheno ~ &#39;,pheno[i],&#39;_&#39;,pT_i), data=pheno_i_covs_prs, family=family_i)
    summary_model&lt;-summary(model)
    
    # Calculate Observed R2, Liability R2 and AUC
    obs_R2&lt;-cor(pheno_i_covs_prs$pheno, pheno_i_covs_prs[[paste0(pheno[i],&#39;_&#39;,pT_i)]])^2
    
    if(family_i == &#39;binomial&#39;){
      library(&#39;fmsb&#39;)
      Nag_R2&lt;-NagR2&lt;-NagelkerkeR2(model)$R2
      h2l_R2N &lt;- function(k, r2n, p) {
        # k baseline disease risk
        # r2n Nagelkerke&#39;s attributable to genomic profile risk score
        # proportion of sample that are cases
        # calculates proportion of variance explained on the liability scale
        #from ABC at http://www.complextraitgenomics.com/software/
        #Lee SH, Goddard ME, Wray NR, Visscher PM. (2012) A better coefficient of determination for genetic profile analysis. Genet Epidemiol. 2012 Apr;36(3):214-24.
        x &lt;- qnorm(1 - k)
        z &lt;- dnorm(x)
        i &lt;- z / k
        cc &lt;- k * (1 - k) * k * (1 - k) / (z^2 * p * (1 - p))
        theta &lt;- i * ((p - k)/(1 - k)) * (i * ((p - k) / ( 1 - k)) - x)
        e &lt;- 1 - p^(2 * p) * (1 - p)^(2 * (1 - p))
        h2l_R2N &lt;- cc * e * r2n / (1 + cc * e * theta * r2n)
      }
      liab_R2&lt;-h2l_R2N(prev[i], Nag_R2, mean(pheno_i_covs_prs$pheno))
      library(pROC)
      auc_prs&lt;-auc(pheno_i_covs_prs$pheno, pheno_i_covs_prs[[paste0(pheno[i],&#39;_&#39;,pT_i)]])
      
        pheno_res&lt;-rbind(pheno_res, data.frame( Phenotype=pheno[i],
                                                Estimate=coef(summary_model)[2,1],
                                                SE=coef(summary_model)[2,2],
                                                P=coef(summary_model)[2,4],
                                                R2_obs=obs_R2,
                                                R2_nag=Nag_R2,
                                                R2_liab=liab_R2,
                                                AUC=auc_prs,
                                                N=length(pheno_i_covs_prs$pheno),
                                                Ncase=sum(pheno_i_covs_prs$pheno == 1),
                                                Ncon=sum(pheno_i_covs_prs$pheno == 0),
                                                Pop_prev=prev[i],
                                                pT=pT_i))
                                              
    } else {
      
      pheno_res&lt;-rbind(pheno_res, data.frame( Phenotype=pheno[i],
                                              Estimate=coef(summary_model)[2,1],
                                              SE=coef(summary_model)[2,2],
                                              P=coef(summary_model)[2,4],
                                              R2_obs=obs_R2,
                                              R2_nag=NA,
                                              R2_liab=NA,
                                              AUC=NA,
                                              N=length(pheno_i_covs_prs$pheno),
                                              Ncase=NA,
                                              Ncon=NA,
                                              Pop_prev=NA,
                                              pT=pT_i))
  
    }
  }
write.csv(pheno_res,paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;), row.names=F, quote=F)
}
</code></pre>
</details>
<hr />
</div>
</div>
<div id="dbslmm" class="section level4">
<h4><span class="header-section-number">2.2.7.3</span> DBSLMM</h4>
<p>I was going to using PRScs auto, but DBSLMM is almost as good and much faster.</p>
<hr />
<div id="run-dbslmm" class="section level5">
<h5><span class="header-section-number">2.2.7.3.1</span> Run DBSLMM</h5>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

gwas=$(echo Depression Intelligence BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer)
pop_prev=$(echo 0.15 NA NA NA 0.05 0.03 0.013 0.00164 0.005 0.125 0.125)
samp_prev=$(echo 0.31654 NA NA NA 0.04444 0.08082 0.0108 0.00355 0.01054 0.04245 0.01705)

# Create directory
mkdir /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM

# Create file listing GWAS that haven&#39;t been processed.
&gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/todo.txt
for i in $(seq 1 11);do
for k in $(seq 1 10);do
gwas_i=$(echo ${gwas} | cut -f ${i} -d &#39; &#39;)
pop_prev_i=$(echo ${pop_prev} | cut -f ${i} -d &#39; &#39;)
sample_prev_i=$(echo ${samp_prev} | cut -f ${i} -d &#39; &#39;)
if [ ! -f /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/${gwas_i}/excl_Subset_${k}/DBSLMM_excl_Subset_${k}_${gwas_i}.EUR.scale ]; then
echo ${gwas_i} ${pop_prev_i} ${sample_prev_i} ${k} &gt;&gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/todo.txt
fi
done
done

# Create shell script to run using sbatch
cat &gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/sbatch.sh &lt;&lt; &#39;EOF&#39;
#!/bin/sh

#SBATCH -p shared,brc
#SBATCH --mem 10G
#SBATCH -J DBSLMM

. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

gwas=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; &#39;NR == var {print $1}&#39; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/todo.txt)
pop_prev=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; &#39;NR == var {print $2}&#39; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/todo.txt)
sample_prev=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; &#39;NR == var {print $3}&#39; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/todo.txt)
subset=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; &#39;NR == var {print $4}&#39; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/todo.txt)

echo ${gwas}
echo ${pop_prev}
echo ${sample_prev}
echo ${subset}

/users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/polygenic_score_file_creator_DBSLMM/polygenic_score_file_creator_DBSLMM.R \
--ref_plink_chr ${Geno_1KG_dir}/1KGPhase3.w_hm3.chr \
--ref_keep ${Geno_1KG_dir}/keep_files/EUR_samples.keep \
--sumstats /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_${subset}_${gwas}.clean.QC.gz \
--plink ${plink1_9} \
--memory 5000 \
--ld_blocks /users/k1806347/brc_scratch/Data/LDetect/EUR \
--rscript /users/k1806347/brc_scratch/Software/Rscript.sh \
--dbslmm /users/k1806347/brc_scratch/Software/DBSLMM/software \
--munge_sumstats ${munge_sumstats} \
--ldsc ${ldsc} \
--ldsc_ref ${ldsc_ref} \
--hm3_snplist ${HapMap3_snplist_dir}/w_hm3.snplist \
--sample_prev ${sample_prev} \
--pop_prev ${pop_prev} \
--output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/${gwas}/excl_Subset_${subset}/DBSLMM_excl_Subset_${subset}_${gwas} \
--ref_pop_scale ${Geno_1KG_dir}/super_pop_keep.list
EOF

sbatch --array 1-$(wc -l /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/todo.txt | cut -d&#39; &#39; -f1)%10 /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/sbatch.sh

sbatch --array 1-1 /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/sbatch.sh

#########################

# Calculate polygenic scores in each subset
for pheno in $(echo Depression Intelligence BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer); do
  for subset in $(seq 1 10);do
  
    mkdir -p /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/DBSLMM/${pheno}/Subset_${subset}
    
    sbatch -n 1 --mem 10G -t 1:00:00 -p brc,shared /users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/Scaled_polygenic_scorer_DBSLMM/Scaled_polygenic_scorer_DBSLMM.R \
      --target_plink_chr /users/k1806347/brc_scratch/Data/UKBB/Genotype/Harmonised/UKBB.w_hm3.QCd.AllSNP.chr \
      --target_keep /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/Subset_${subset}.keep \
      --ref_score /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/${pheno}/excl_Subset_${subset}/DBSLMM_excl_Subset_${subset}_${pheno}.dbslmm.GW.txt \
      --ref_scale /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/DBSLMM/${pheno}/excl_Subset_${subset}/DBSLMM_excl_Subset_${subset}_${pheno}.EUR.scale \
      --ref_freq_chr /users/k1806347/brc_scratch/Data/1KG/Phase3/freq_files/EUR/1KGPhase3.w_hm3.EUR.chr \
      --pheno_name ${pheno} \
      --plink /users/k1806347/brc_scratch/Software/plink1.9.sh \
      --output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/DBSLMM/${pheno}/Subset_${subset}/Subset_${subset}_${pheno}_DBSLMM
  done
done
</code></pre>
</details>
<hr />
</div>
<div id="evaluate-predictive-utility-of-dbslmm" class="section level5">
<h5><span class="header-section-number">2.2.7.3.2</span> Evaluate predictive utility of DBSLMM</h5>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code># Combine the polygenic scores for each subset
library(data.table)

####
# Prepare covariate data
####

# Read in covariate data
geno_covar&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/ukb18177_glanville_covariates.UpdateIDs.txt&#39;)
sex&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Sex.pheno&#39;)
age&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Age.pheno&#39;)

covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(geno_covar, sex, age))

# Convert catagorical variables into factors so they are dummy coded in the regression
covs$batch&lt;-factor(covs$batch)
covs$assessment_centre&lt;-factor(covs$assessment_centre)

# Read in list of individuals passing QC
QC&lt;-fread(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/ukb18177_glanville_post_qc_id_list.UpdateIDs.fam&#39;)
names(QC)&lt;-c(&#39;FID&#39;,&#39;IID&#39;)

# Merge all data
covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(covs, QC))

###
# Read in pheno type and PRS data, regress covariates from PRS within subset, and the estimate R2 and AUC across subsets.
###

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
pheno_file&lt;-c(&#39;ever_depressed_pheno_final.UpdateIDs.txt&#39;,&#39;UKBB_Fluid.intelligence.score.UpdateIDs.pheno&#39;,&#39;UKBB_BMI.score.UpdateIDs.pheno&#39;,&#39;UKBB_Height.score.UpdateIDs.pheno&#39;,&#39;t2d_only_111119.UpdateIDs.txt&#39;,&#39;cad_only_111119.UpdateIDs.txt&#39;,&#39;UKBB.IBD.txt&#39;,&#39;UKBB.MultiScler.txt&#39;,&#39;UKBB.RheuArth.txt&#39;,&#39;UKBB_Breast_Cancer.pheno&#39;,&#39;UKBB_Prostate_Cancer.pheno&#39;)
prev=c(0.15,NA,NA,NA,0.05,0.03,0.013,0.00164,0.005,0.125,0.125)

pheno_res&lt;-NULL
for(i in c(1:11)){
  print(pheno[i])
  
  # Read in phenotype file
  pheno_i&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/&#39;,pheno[i],&#39;/&#39;,pheno_file[i]))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  
  # Determine whether binary
  if(length(unique(pheno_i$pheno[!is.na(pheno_i$pheno)])) == 2){
    family_i&lt;-&#39;binomial&#39;
  } else {
    family_i&lt;-&#39;gaussian&#39;
  }
  
  # Merge pheno and covar data
  pheno_i_covs&lt;-merge(pheno_i, covs, by=c(&#39;FID&#39;,&#39;IID&#39;), all=T)

  pheno_i_covs_prs&lt;-NULL
  for(subset_i in 1:10){
    # Read in PRS for subset      
    prs_var_subset&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/DBSLMM/&#39;,pheno[i],&#39;/Subset_&#39;,subset_i,&#39;/Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;_DBSLMM.DBSLMM_profiles&#39;))
    names(prs_var_subset)&lt;-c(&#39;FID&#39;,&#39;IID&#39;,&#39;DBSLMM&#39;)
    
    # Merge with pheno and covs
    pheno_i_covs_prs_subset&lt;-merge(pheno_i_covs, prs_var_subset, by=c(&#39;FID&#39;,&#39;IID&#39;))
    
    # Remove incomplete rows
    pheno_i_covs_prs_subset&lt;-pheno_i_covs_prs_subset[complete.cases(pheno_i_covs_prs_subset),]

    # Regress covariates out of the PRS
    pheno_i_covs_prs_subset$DBSLMM&lt;-as.numeric(scale(resid(glm(DBSLMM ~ ., family=&#39;gaussian&#39;, data=pheno_i_covs_prs_subset[,-1:-3]))))
    
    pheno_i_covs_prs&lt;-rbind(pheno_i_covs_prs, pheno_i_covs_prs_subset)
  }
  
  # Run regression between outcome and PRS residuals
  model&lt;-glm(pheno ~ DBSLMM, data=pheno_i_covs_prs, family=family_i)
  summary_model&lt;-summary(model)
  
  # Calculate Observed R2, Liability R2 and AUC
  obs_R2&lt;-cor(pheno_i_covs_prs$pheno, pheno_i_covs_prs$DBSLMM)^2
  
  if(family_i == &#39;binomial&#39;){
    library(&#39;fmsb&#39;)
    Nag_R2&lt;-NagR2&lt;-NagelkerkeR2(model)$R2
    h2l_R2N &lt;- function(k, r2n, p) {
      # k baseline disease risk
      # r2n Nagelkerke&#39;s attributable to genomic profile risk score
      # proportion of sample that are cases
      # calculates proportion of variance explained on the liability scale
      #from ABC at http://www.complextraitgenomics.com/software/
      #Lee SH, Goddard ME, Wray NR, Visscher PM. (2012) A better coefficient of determination for genetic profile analysis. Genet Epidemiol. 2012 Apr;36(3):214-24.
      x &lt;- qnorm(1 - k)
      z &lt;- dnorm(x)
      i &lt;- z / k
      cc &lt;- k * (1 - k) * k * (1 - k) / (z^2 * p * (1 - p))
      theta &lt;- i * ((p - k)/(1 - k)) * (i * ((p - k) / ( 1 - k)) - x)
      e &lt;- 1 - p^(2 * p) * (1 - p)^(2 * (1 - p))
      h2l_R2N &lt;- cc * e * r2n / (1 + cc * e * theta * r2n)
    }
    liab_R2&lt;-h2l_R2N(prev[i], Nag_R2, mean(pheno_i_covs_prs$pheno))
    library(pROC)
    auc_prs&lt;-auc(pheno_i_covs_prs$pheno, pheno_i_covs_prs$DBSLMM)
    
      pheno_res&lt;-rbind(pheno_res, data.frame( Phenotype=pheno[i],
                                              Estimate=coef(summary_model)[2,1],
                                              SE=coef(summary_model)[2,2],
                                              P=coef(summary_model)[2,4],
                                              R2_obs=obs_R2,
                                              R2_nag=Nag_R2,
                                              R2_liab=liab_R2,
                                              AUC=auc_prs,
                                              N=length(pheno_i_covs_prs$pheno),
                                              Ncase=sum(pheno_i_covs_prs$pheno == 1),
                                              Ncon=sum(pheno_i_covs_prs$pheno == 0),
                                              Pop_prev=prev[i]))
                                            
  } else {
    
    pheno_res&lt;-rbind(pheno_res, data.frame( Phenotype=pheno[i],
                                            Estimate=coef(summary_model)[2,1],
                                            SE=coef(summary_model)[2,2],
                                            P=coef(summary_model)[2,4],
                                            R2_obs=obs_R2,
                                            R2_nag=NA,
                                            R2_liab=NA,
                                            AUC=NA,
                                            N=length(pheno_i_covs_prs$pheno),
                                            Ncase=NA,
                                            Ncon=NA,
                                            Pop_prev=NA))

  }
}

write.csv(pheno_res, &#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/DBSLMM/Pheno_PRS_assoc_res.csv&#39;, row.names=F, quote=F)</code></pre>
</details>
<hr />
</div>
</div>
</div>
<div id="compare-observed-variance-explained-measurements-to-those-derived-using-avengeme-and-ldsc" class="section level3">
<h3><span class="header-section-number">2.2.8</span> Compare observed variance explained measurements to those derived using AVENGEME and LDSC</h3>
<p>AVENGEME can estimate the R2 you would expect to see from a polygenic scores given an estimate of heritability and sample size. To estimate heritability and polygenicity, we could use GCTB.</p>
<hr />
<div id="estimate-the-snp-based-heritability-using-ldsc" class="section level4">
<h4><span class="header-section-number">2.2.8.1</span> Estimate the SNP-based heritability using LDSC</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>dir.create(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/LDSC&#39;)

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
prev=c(0.15,NA,NA,NA,0.05,0.03,0.013,0.00164,0.005,0.125,0.125)

library(data.table)

for(i in 1:length(pheno)){
  for(subset_i in 1:10){
      system(paste0(&#39;~/brc_scratch/Software/munge_sumstats.sh --sumstats /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.clean.QC.gz --merge-alleles /users/k1806347/brc_scratch/Data/ldsc/w_hm3.snplist --out /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.clean.QC.munged&#39;))
  }
}

for(i in 1:length(pheno)){
  PRS_res&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;)) 
  sample_prev&lt;-round(PRS_res$Ncase[1]/PRS_res$N[1],5)
  
  for(subset_i in 1:10){
    if(is.na(prev[i])){
      system(paste0(&#39;~/brc_scratch/Software/ldsc.sh --h2 /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.clean.QC.munged.sumstats.gz --ref-ld-chr /users/k1806347/brc_scratch/Data/ldsc/eur_w_ld_chr/ --w-ld-chr /users/k1806347/brc_scratch/Data/ldsc/eur_w_ld_chr/ --out /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/LDSC/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.obs&#39;))
    } else {
      system(paste0(&#39;~/brc_scratch/Software/ldsc.sh --h2 /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.clean.QC.munged.sumstats.gz --ref-ld-chr /users/k1806347/brc_scratch/Data/ldsc/eur_w_ld_chr/ --w-ld-chr /users/k1806347/brc_scratch/Data/ldsc/eur_w_ld_chr/ --samp-prev &#39;,sample_prev,&#39; --pop-prev &#39;,prev[i],&#39; --out /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/LDSC/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.liab&#39;))
    }
  }
}

# Read in LDSC estimates meta-analyse
library(meta)

meta_h2&lt;-NULL
for(i in 1:length(pheno)){
  if(is.na(prev[i])){
    pheno_ldsc&lt;-NULL
    PRS_res&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;)) 
    sample_N&lt;-PRS_res$N[1]
    for(subset_i in 1:10){
      log&lt;-read.table(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/LDSC/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.obs.log&#39;), sep=&#39;&amp;&#39;)$V1
      int_log&lt;-as.character(log[grepl(&#39;Intercept:&#39;, log)])
      int_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;,gsub(&#39;.*: &#39;, &#39;&#39;, int_log)))
      int_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;, &#39;&#39;, int_log)))
      h2_log&lt;-as.character(log[grepl(&#39;Total Observed scale h2&#39;, log)])
      h2_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;,gsub(&#39;.*: &#39;, &#39;&#39;, h2_log)))
      h2_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;, &#39;&#39;, h2_log)))
      
      pheno_ldsc&lt;-rbind(pheno_ldsc, data.frame(Est=h2_est,
                                               SE=h2_se,
                                               Int=int_est,
                                               Int_SE=int_se,
                                               Subset=subset_i))
    }
    meta_pheno_ldsc&lt;-metagen(pheno_ldsc$Est, pheno_ldsc$SE, pheno_ldsc$Subset, comb.random=F)
    meta_pheno_ldsc_int&lt;-metagen(pheno_ldsc$Int, pheno_ldsc$Int_SE, pheno_ldsc$Subset, comb.random=F)
    
  meta_h2&lt;-rbind(meta_h2,data.frame(Phenotype=pheno[i],
                            LDSC_int=meta_pheno_ldsc_int$TE.fixed,
                            H2_liab=NA,
                            H2_obs=meta_pheno_ldsc$TE.fixed,
                            SE=meta_pheno_ldsc$seTE.fixed,
                            P=meta_pheno_ldsc$pval.fixed,
                            pop_prev=NA,
                            sample_prev=NA,
                            N=sample_N))

  } else {
    pheno_ldsc&lt;-NULL
    PRS_res&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;)) 
    sample_prev&lt;-round(PRS_res$Ncase[1]/PRS_res$N[1],5)
    sample_N&lt;-PRS_res$N[1]
  
    for(subset_i in 1:10){
      log&lt;-read.table(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/LDSC/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.liab.log&#39;), sep=&#39;&amp;&#39;)$V1
      int_log&lt;-as.character(log[grepl(&#39;Intercept:&#39;, log)])
      int_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;,gsub(&#39;.*: &#39;, &#39;&#39;, int_log)))
      int_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;, &#39;&#39;, int_log)))
      h2_log&lt;-as.character(log[grepl(&#39;Total Liability scale h2&#39;, log)])
      h2_est&lt;-as.numeric(gsub(&#39; .*&#39;,&#39;&#39;,gsub(&#39;.*: &#39;, &#39;&#39;, h2_log)))
      h2_se&lt;-as.numeric(gsub(&quot;\\)&quot;,&#39;&#39;,gsub(&quot;.*\\(&quot;, &#39;&#39;, h2_log)))
      
      pheno_ldsc&lt;-rbind(pheno_ldsc, data.frame(Est=h2_est,
                                               SE=h2_se,
                                               Int=int_est,
                                               Int_SE=int_se,
                                               Subset=subset_i))
    }
    meta_pheno_ldsc&lt;-metagen(pheno_ldsc$Est, pheno_ldsc$SE, pheno_ldsc$Subset, comb.random=F)
    meta_pheno_ldsc_int&lt;-metagen(pheno_ldsc$Int, pheno_ldsc$Int_SE, pheno_ldsc$Subset, comb.random=F)

  meta_h2&lt;-rbind(meta_h2,data.frame(Phenotype=pheno[i],
                            LDSC_int=meta_pheno_ldsc_int$TE.fixed,
                            H2_liab=meta_pheno_ldsc$TE.fixed,
                            H2_obs=NA,
                            SE=meta_pheno_ldsc$seTE.fixed,
                            P=meta_pheno_ldsc$pval.fixed,
                            pop_prev=prev[i],
                            sample_prev=sample_prev,
                            N=sample_N))
  }
}

write.csv(meta_h2, &#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/LDSC/meta_h2.res.csv&#39;,row.names=F, quote=F)

# The Height LDSC intercept is very high.</code></pre>
</details>
<hr />
</div>
<div id="estimate-auc-using-avengeme" class="section level4">
<h4><span class="header-section-number">2.2.8.2</span> Estimate AUC using AVENGEME</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(avengeme)

# pi0 parameter doesn&#39;t affect AUC why using a p-value threshold of 1.

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)

meta_h2&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/LDSC/meta_h2.res.csv&#39;)

AUC_R2_pred&lt;-NULL
for(i in 1:length(pheno)){
  
  pheno_nsnp&lt;-NULL
  for(subset_i in 1:10){
    tmp&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/pTclump/&#39;,pheno[i],&#39;/excl_Subset_&#39;,subset_i,&#39;/pTclump_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.NSNP_per_pT&#39;))
  
    pheno_nsnp&lt;-rbind(pheno_nsnp, data.frame(Subset=subset_i,
                           NSNP=tmp$NSNP[length(tmp$NSNP)]))
  }

  mean_nsnp&lt;-mean(pheno_nsnp$NSNP)
  
  if(is.na(meta_h2$H2_liab[meta_h2$Phenotype == pheno[i]])){
    tmp&lt;-polygenescore(nsnp=mean_nsnp, n=meta_h2$N[meta_h2$Phenotype == pheno[i]], vg1 = meta_h2$H2_obs[meta_h2$Phenotype == pheno[i]], pupper = c(0, 1), pi0=0.9, nested = TRUE, weighted = TRUE, binary = F)
    
    pheno_res_pTclump&lt;-read.csv(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;))
    
    pT&lt;-as.numeric(gsub(&#39;_&#39;,&#39;-&#39;,pheno_res_pTclump$pT))
  
    tmp2&lt;-estimatePolygenicModel(p=pheno_res_pTclump$Estimate/pheno_res_pTclump$SE, 
                           nsnp=mean_nsnp, 
                           n=c(meta_h2$N[meta_h2$Phenotype == pheno[i]], pheno_res_pTclump$N[1]), 
                           pupper = c(0,pT),
                           binary = c(FALSE, FALSE), 
                           prevalence = c(NA, NA), 
                           sampling = c(NA, NA), 
                           fixvg2pi02 = T,
                           alpha = 0.05)
    
    tmp3&lt;-polygenescore(nsnp=mean_nsnp, n=meta_h2$N[meta_h2$Phenotype == pheno[i]], vg1 = tmp2$vg[1], pupper = c(0, 1), pi0=tmp2$pi0[1], nested = TRUE, weighted = TRUE, binary = F)

    AUC_R2_pred&lt;-rbind(AUC_R2_pred, data.frame(Phenotype=pheno[i],
                                         N=meta_h2$N[meta_h2$Phenotype == pheno[i]],
                                         LDSC_int=meta_h2$LDSC_int[meta_h2$Phenotype == pheno[i]],
                                         LDSC_vg1=meta_h2$H2_obs[meta_h2$Phenotype == pheno[i]],
                                         AVENGEME_vg1=tmp2$vg[1],
                                         AVENGEME_pi0=tmp2$pi0[1],
                                         prevalence=meta_h2$pop_prev[meta_h2$Phenotype == pheno[i]],
                                         sampling=meta_h2$sample_prev[meta_h2$Phenotype == pheno[i]],
                                         NSNP=mean_nsnp,
                                         pred_AUC=NA,
                                         pred_R2=tmp$R2,
                                         pred_AUC_AVENG=NA,
                                         pred_R2_AVENG=tmp3$R2,
                                         pTclump_AUC=NA,
                                         pTclump_R2=pheno_res_pTclump$R2_obs[pheno_res_pTclump$pT == &#39;1&#39;]))
  
  } else {
    
    tmp&lt;-polygenescore(nsnp=mean_nsnp, n=meta_h2$N[meta_h2$Phenotype == pheno[i]], vg1 = meta_h2$H2_liab[meta_h2$Phenotype == pheno[i]], pupper = c(0, 1), pi0=0.9, nested = TRUE, weighted = TRUE, binary = T, prevalence = meta_h2$pop_prev[meta_h2$Phenotype == pheno[i]], sampling = meta_h2$sample_prev[meta_h2$Phenotype == pheno[i]])
    
    pheno_res_pTclump&lt;-read.csv(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;))
  
    pT&lt;-as.numeric(gsub(&#39;_&#39;,&#39;-&#39;,pheno_res_pTclump$pT))

    tmp2&lt;-estimatePolygenicModel(p=pheno_res_pTclump$Estimate/pheno_res_pTclump$SE, 
                           nsnp=mean_nsnp, 
                           n=c(meta_h2$N[meta_h2$Phenotype == pheno[i]], pheno_res_pTclump$N[1]), 
                           pupper = c(0,pT),
                           binary = T, 
                           prevalence = meta_h2$pop_prev[meta_h2$Phenotype == pheno[i]], 
                           sampling = meta_h2$sample_prev[meta_h2$Phenotype == pheno[i]], 
                           fixvg2pi02 = T,
                           alpha = 0.05)
    
    tmp3&lt;-polygenescore(nsnp=mean_nsnp, n=meta_h2$N[meta_h2$Phenotype == pheno[i]], vg1 = tmp2$vg[1], pupper = c(0, 1), pi0=tmp2$pi0[1], nested = TRUE, weighted = TRUE, binary = T)

    AUC_R2_pred&lt;-rbind(AUC_R2_pred, data.frame(Phenotype=pheno[i],
                                         N=meta_h2$N[meta_h2$Phenotype == pheno[i]],
                                         LDSC_int=meta_h2$LDSC_int[meta_h2$Phenotype == pheno[i]],
                                         LDSC_vg1=meta_h2$H2_liab[meta_h2$Phenotype == pheno[i]],
                                         AVENGEME_vg1=tmp2$vg[1],
                                         AVENGEME_pi0=tmp2$pi0[1],

                                         prevalence=meta_h2$pop_prev[meta_h2$Phenotype == pheno[i]],
                                         sampling=meta_h2$sample_prev[meta_h2$Phenotype == pheno[i]],
                                         NSNP=mean_nsnp,
                                         pred_AUC=tmp$AUC,
                                         pred_R2=NA,
                                         pred_AUC_AVENG=tmp3$AUC,
                                         pred_R2_AVENG=NA,

                                         pTclump_AUC=pheno_res_pTclump$AUC[pheno_res_pTclump$pT == &#39;1&#39;],
                                         pTclump_R2=NA))
  }
}

write.csv(AUC_R2_pred, &#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/AUC_predictions.csv&#39;, row.names=F, quote=F)

# The estimates of h2 are similar between LDSC and AVENGEME, except for LDSC, for which the LDSC intercept is very high (1.4).
# The predicted AUC/R2 is generally lower than the observed. This could reflect the homogeneity of UKB leading to better R2 than expected.
# This uses the pT of 1, but this is no optimal for less polygenic outcomes, and will therefore drastically underestimate the AUC/R2 of PRS derived using methods that account for varying polygenicity (i.e. PRScs auto, DBSLMM etc.). Incorporating a polygenicity parameter would allow us to account for this, but estimating polygenicity using summary statistics is not easy, and I have found discordance between AVENGEME and SBayesR in the past. Even when setting pi0 to the AVENGME estimate, the estimated AUC/R2 of lower pT is different from the observed.
# I don&#39;t think the AVENGEME approach is going to work unless we only using pT=1 pT+clump PRS, which are suboptimal.
# The GWAS ROCS approach could work better. I used this before and found it was very slow, but didn&#39;t give it a proper chance.
# Should probably just provide the absolute estimate for a subset of GWAS where we have manually extracted the R2 of the PRS.</code></pre>
</details>
<hr />
</div>
</div>
<div id="estimate-auc-using-gwiz" class="section level3">
<h3><span class="header-section-number">2.2.9</span> Estimate AUC using GWIZ</h3>
<p>We will calculate the AUC using GWIZ using the leave one out meta-analysis summary statistics, and then average the AUC results. This will hopefully tell us how much variance GWAS expects the LOO sumstats to explain in the left out subset.</p>
<hr />
<div id="compute-allele-frequencies" class="section level4">
<h4><span class="header-section-number">2.2.9.1</span> Compute allele frequencies</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>for chr in $(seq 1 22); do
    sbatch -p brc,shared --mem 5G -t 00:10:00 /users/k1806347/brc_scratch/Software/plink1.9.sh \
      --bfile /users/k1806347/brc_scratch/Data/UKBB/Genotype/Harmonised/UKBB.w_hm3.QCd.AllSNP.chr${chr} \
      --keep /users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/ukb18177_glanville_post_qc_id_list.UpdateIDs.fam \
      --maf 0.01 \
      --geno 0.05 \
      --freq \
      --allow-no-sex \
      --out /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/allele_freq_chr${chr}
done</code></pre>
</details>
<hr />
</div>
<div id="format-gwas-summary-statistics-1" class="section level4">
<h4><span class="header-section-number">2.2.9.2</span> Format GWAS summary statistics</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>library(data.table)

pheno&lt;-c(&#39;Depression&#39;,&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)
pheno_file&lt;-c(&#39;ever_depressed_pheno_final.UpdateIDs.txt&#39;,&#39;UKBB_Fluid.intelligence.score.UpdateIDs.pheno&#39;,&#39;UKBB_BMI.score.UpdateIDs.pheno&#39;,&#39;UKBB_Height.score.UpdateIDs.pheno&#39;,&#39;t2d_only_111119.UpdateIDs.txt&#39;,&#39;cad_only_111119.UpdateIDs.txt&#39;,&#39;UKBB.IBD.txt&#39;,&#39;UKBB.MultiScler.txt&#39;,&#39;UKBB.RheuArth.txt&#39;,&#39;UKBB_Breast_Cancer.pheno&#39;,&#39;UKBB_Prostate_Cancer.pheno&#39;)

# Read in covariate data
geno_covar&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/ukb18177_glanville_covariates.UpdateIDs.txt&#39;)
sex&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Sex.pheno&#39;)
age&lt;-fread(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/UKBB_Age.pheno&#39;)
covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(geno_covar, sex, age))
# Convert catagorical variables into factors so they are dummy coded in the regression
covs$batch&lt;-factor(covs$batch)
covs$assessment_centre&lt;-factor(covs$assessment_centre)
# Read in list of individuals passing QC
QC&lt;-fread(&#39;/users/k1806347/brc_scratch/Analyses/PRS_comparison/UKBB_outcomes_for_prediction/ukb18177_glanville_post_qc_id_list.UpdateIDs.fam&#39;)
names(QC)&lt;-c(&#39;FID&#39;,&#39;IID&#39;)
# Merge all data
covs&lt;-Reduce(function(...) merge(..., all=F, by=c(&#39;FID&#39;,&#39;IID&#39;)), list(covs, QC))
# Read in allele frequency per variant
freq&lt;-NULL
for(chr in 1:22){
  freq&lt;-rbind(freq, fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/allele_freq_chr&#39;,chr,&#39;.frq&#39;)))
}
freq$A1&lt;-tolower(freq$A1)
freq$A2&lt;-tolower(freq$A2)

for(i in c(1,5:11)){
  # Read in pheno file to determine number of cases and controls
  pheno_i&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/Phenotype/&#39;,pheno[i],&#39;/&#39;,pheno_file[i]))
  names(pheno_i)[3]&lt;-&#39;pheno&#39;
  
  # Merge pheno and covar data
  pheno_i_covs&lt;-merge(pheno_i, covs, by=c(&#39;FID&#39;,&#39;IID&#39;), all=T)
  pheno_i_covs&lt;-pheno_i_covs[complete.cases(pheno_i_covs),]
  
  # Identify proportion of cases and controls which can be used to estimate the number of cases and controls in each LOO meta-analysis.
  sample_prev&lt;-mean(pheno_i_covs$pheno)
  
  for(subset_i in 1:10){
    # Read in meta-analysis GWAS sumstats
    gwas&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;1.tbl&#39;))
    names(gwas)&lt;-c(&#39;SNP&#39;,&#39;A1&#39;,&#39;A2&#39;,&#39;BETA&#39;,&#39;SE&#39;,&#39;P&#39;,&#39;Direction&#39;)
    gwas$OR&lt;-exp(gwas$BETA)
    gwas$phenotype&lt;-&#39;n/a&#39;
    gwas$dataset&lt;-&#39;n/a&#39;
    gwas$model&lt;-&#39;additive&#39;
    
    # Read sample size per variant
    N_tab&lt;-fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/N_table_&#39;,pheno[i],&#39;.txt&#39;))
    
    N_tab$excl_subset&lt;-rowSums(N_tab[,names(N_tab)[!grepl(paste0(&#39;SNP|Subset_&#39;,subset_i,&#39;_N&#39;), names(N_tab))],with=F])
  
    N_tab$control_size&lt;-round(N_tab$excl_subset*(1-sample_prev))
    N_tab$case_size&lt;-round(N_tab$excl_subset*(sample_prev))
      
    gwas&lt;-merge(gwas, N_tab[,c(&#39;SNP&#39;,&#39;control_size&#39;,&#39;case_size&#39;), with=F], by=&#39;SNP&#39;)
    
    # Merge frequency data
    gwas_same&lt;-merge(gwas, freq[,c(&#39;SNP&#39;,&#39;A1&#39;,&#39;A2&#39;,&#39;MAF&#39;)], by=c(&#39;SNP&#39;,&#39;A1&#39;,&#39;A2&#39;))
    gwas_flip&lt;-merge(gwas, freq[,c(&#39;SNP&#39;,&#39;A1&#39;,&#39;A2&#39;,&#39;MAF&#39;)], by.x=c(&#39;SNP&#39;,&#39;A1&#39;,&#39;A2&#39;),by.y=c(&#39;SNP&#39;,&#39;A2&#39;,&#39;A1&#39;))
    gwas_flip$MAF&lt;-1-gwas_flip$MAF
    gwas&lt;-rbind(gwas_same, gwas_flip)
    
    # OR and MAFs to all be corresponding to the risk allele
    gwas$MAF[gwas$OR &lt; 1]&lt;-1-gwas$MAF[gwas$OR &lt; 1]
    names(gwas)[names(gwas) == &#39;MAF&#39;]&lt;-&#39;risk_allele_freq&#39;
    gwas$OR[gwas$OR &lt; 1]&lt;-exp(-gwas$BETA[gwas$OR &lt; 1])
    
    # Extract LD independent variants
    ld_indep&lt;-NULL
    for(chr in 1:22){
      ld_indep&lt;-rbind(ld_indep, fread(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/pTclump/&#39;,pheno[i],&#39;/excl_Subset_&#39;,subset_i,&#39;/pTclump_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.chr&#39;,chr,&#39;.range_values&#39;)))
    }
    
    # Filter genome-wide variants 
    ld_indep_sig&lt;-ld_indep[ld_indep$V2 &lt; 1e-6]
    gwas&lt;-gwas[(gwas$SNP %in% ld_indep_sig$V1),]
    gwas&lt;-gwas[,c(&#39;phenotype&#39;,&#39;dataset&#39;,&#39;SNP&#39;,&#39;control_size&#39;,&#39;case_size&#39;,&#39;risk_allele_freq&#39;,&#39;OR&#39;,&#39;model&#39;),with=F]
    names(gwas)[names(gwas) == &#39;SNP&#39;]&lt;-&#39;accession&#39;
    
    gwas$model&lt;-rep(c(&#39;recessive&#39;,&#39;dominant&#39;),dim(gwas)[1])[1:dim(gwas)[1]]
    
      fwrite(gwas, paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/GWIZ/metal_excl_Subset_&#39;,subset_i,&#39;_&#39;,pheno[i],&#39;.GWIZ.csv&#39;), sep=&#39;,&#39;, na=&#39;NA&#39;, quote=F)
  }
}</code></pre>
</details>
<hr />
</div>
<div id="run-gwiz-1" class="section level4">
<h4><span class="header-section-number">2.2.9.3</span> Run GWIZ</h4>
<p>GWIZ is an R script. It needs to modification: - It doesn’t allow input files, software and output to be in different directories - It uses the png function which does’t work on Rosalind - It prints the contents of the GWAS sumstats repeatedly and GWIZ is an Rscript that must be modified to allow for different input and output files. I have edited the script slightly so all files don’t have be stored in the same directory, and can be easily run in parallel.</p>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code># Run GWIZ for each phenotype and subset
for pheno in $(echo Depression T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer); do
  for subset in $(seq 1 10); do
    sbatch -p brc,shared --mem 10G /users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/gwizer/gwizer.R \
      --sumstats /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/GWIZ/metal_excl_Subset_${subset}_${pheno}.GWIZ.csv \
      --gwiz /mnt/lustre/users/k1806347/Software/GWIZ-Rscript-master \
      --output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/GWIZ/${pheno}/Subset_${subset}/GWIZ_${pheno}_Subset_${subset}
  done
done


/users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/gwizer/gwizer.R \
      --sumstats ~/Test.csv \
      --gwiz /mnt/lustre/users/k1806347/Software/GWIZ-Rscript-master \
      --output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/GWIZ/Test/Test</code></pre>
</details>
<hr />
</div>
<div id="compare-observed-and-estimate-auc-2" class="section level4">
<h4><span class="header-section-number">2.2.9.4</span> Compare observed and estimate AUC</h4>
<details>
<p><summary>Show code</summary></p>
<pre class="r"><code>pheno&lt;-c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)

AUC_comp&lt;-NULL
for(i in 1:length(pheno)){
  GWIZ_AUC_i&lt;-NULL
  for(k in 1:10){
    GWIZ_AUC_i_k&lt;-read.table(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/GWIZ/&#39;,pheno[i],&#39;/Subset_&#39;,k,&#39;/GWIZ_&#39;,pheno[i],&#39;_Subset_&#39;,k,&#39;.log&#39;), sep=&#39;&amp;&#39;, header=F)$V1
    GWIZ_AUC_i_k_auc&lt;-as.numeric(gsub(&#39;AUC = &#39;,&#39;&#39;,GWIZ_AUC_i_k[grepl(&#39;AUC&#39;, GWIZ_AUC_i_k)]))
    GWIZ_AUC_i&lt;-rbind(GWIZ_AUC_i, data.frame(Subset=k,
                                             AUC=GWIZ_AUC_i_k_auc))
  }
  GWIZ_AUC_i_average&lt;-mean(GWIZ_AUC_i$AUC)
  
  pheno_res_pTclump&lt;-read.csv(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;))
  pheno_res_pTclump$pT&lt;-as.numeric(gsub(&#39;_&#39;,&#39;-&#39;,pheno_res_pTclump$pT))
  Obs_AUC&lt;-pheno_res_pTclump$AUC[pheno_res_pTclump$pT == 1e-6]
  
  AUC_comp&lt;-rbind(AUC_comp, data.frame(Phenotype=pheno[i],
                                       Obs_AUC=Obs_AUC,
                                       GWIZ_AUC=GWIZ_AUC_i_average))
}

# GWIZ is performing terribly. It thinks the AUC is ~0.5 for all outcomes. I feel like I am doing something wrong when running GWIZ. It works when I use the GWIZ example sumstats, indicating the problem is how I derived the GWAS sumstats or formatted them. I reckon it is because the OR are really log(BETA), and therefore not to scale. This would be clearer, if I used external GWAS sumstats.</code></pre>
</details>
<hr />
</div>
</div>
<div id="lassosum-1" class="section level3">
<h3><span class="header-section-number">2.2.10</span> Lassosum</h3>
<details>
<p><summary>Show code</summary></p>
<pre class="bash"><code>. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

# Create directory
mkdir /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum

# Create file listing GWAS that haven&#39;t been processed.
&gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/todo.txt
for pheno in $(echo Depression Intelligence BMI Height T2D CAD IBD MultiScler RheuArth Breast_Cancer Prostate_Cancer);do
for subset in $(seq 1 10);do
if [ ! -f /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/${pheno}/lassosum_pseudo_${pheno}.pseudoval.txt ]; then
echo $pheno $subset &gt;&gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/todo.txt
fi
done
done

# Create shell script to run using sbatch
cat &gt; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/sbatch.sh &lt;&lt; &#39;EOF&#39;
#!/bin/sh

#SBATCH -p shared,brc
#SBATCH --mem=20G
#SBATCH -n 2
#SBATCH -J lassosum

. /users/k1806347/brc_scratch/Software/MyGit/GenoPred/config_used/Pipeline_prep.config

pheno=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; &#39;NR == var {print $1}&#39; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/todo.txt)
subset=$(awk -v var=&quot;$SLURM_ARRAY_TASK_ID&quot; &#39;NR == var {print $2}&#39; /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/todo.txt)

/users/k1806347/brc_scratch/Software/Rscript.sh /users/k1806347/brc_scratch/Software/MyGit/GenoPred/Scripts/lassosum_pseudovalidate/lassosum_pseudovalidate.R \
    --ref_plink_gw ${Geno_1KG_dir}/1KGPhase3.w_hm3.GW \
    --ref_keep ${Geno_1KG_dir}/keep_files/EUR_samples.keep \
  --sumstats /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/metal_excl_Subset_${subset}_${pheno}.clean.QC.gz \
    --output /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/${pheno}/Subset_${subset}/lassosum_pseudo_${pheno}_Subset_${subset} \
    --n_cores 2
EOF

sbatch --array 1-$(wc -l /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/todo.txt | cut -d&#39; &#39; -f1)%5 /users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/sbatch.sh
</code></pre>
</details>
<hr />
<div id="compare-observed-and-estimate-auc-3" class="section level4">
<h4><span class="header-section-number">2.2.10.1</span> Compare observed and estimate AUC</h4>
<details>
<p><summary>Show results</summary></p>
<pre class="r"><code>pheno&lt;-c(&#39;Depression&#39;,&#39;T2D&#39;,&#39;CAD&#39;,&#39;IBD&#39;,&#39;MultiScler&#39;,&#39;RheuArth&#39;,&#39;Breast_Cancer&#39;,&#39;Prostate_Cancer&#39;)

AUC_comp&lt;-NULL
for(i in 1:length(pheno)){
  lassosum_AUC_i&lt;-NULL
  for(k in 1:10){
    lassosum_AUC_i_k&lt;-read.table(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/&#39;,pheno[i],&#39;/Subset_&#39;,k,&#39;/lassosum_pseudo_&#39;,pheno[i],&#39;_Subset_&#39;,k,&#39;.log&#39;), sep=&#39;&amp;&#39;, header=F)$V1
    
    lasso_AUC_i_k_r&lt;-as.numeric(gsub(&#39;value = &#39;,&#39;&#39;,lassosum_AUC_i_k[grepl(&#39;value = &#39;, lassosum_AUC_i_k)]))
    lasso_AUC_i_k_d&lt;- ((2*lasso_AUC_i_k_r)/sqrt(1-lasso_AUC_i_k_r^2))
    lasso_AUC_i_k_auc &lt;- pnorm(lasso_AUC_i_k_d/sqrt(2), 0, 1)

    lassosum_AUC_i&lt;-rbind(lassosum_AUC_i, data.frame( Subset=k,
                                                      AUC=lasso_AUC_i_k_auc))
  }
  
  lassosum_AUC_i_average&lt;-mean(lassosum_AUC_i$AUC)
  
  pheno_res_pTclump&lt;-read.csv(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;))
  pheno_res_pTclump$pT&lt;-as.numeric(gsub(&#39;_&#39;,&#39;-&#39;,pheno_res_pTclump$pT))
  Obs_AUC_pTclump&lt;-pheno_res_pTclump$AUC[pheno_res_pTclump$pT == 1]
  
  pheno_res_DBSLMM&lt;-read.csv(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/DBSLMM/Pheno_PRS_assoc_res.csv&#39;))
  pheno_res_DBSLMM&lt;-pheno_res_DBSLMM[pheno_res_DBSLMM$Phenotype == pheno[i],]
  Obs_AUC_DBSLMM&lt;-pheno_res_DBSLMM$AUC
  
  AUC_comp&lt;-rbind(AUC_comp, data.frame(Phenotype=pheno[i],
                                       pTclump_AUC=Obs_AUC_pTclump,
                                       DBSLMM_AUC=Obs_AUC_DBSLMM,
                                       lassosum_AUC=lassosum_AUC_i_average))
}

AUC_comp$diff&lt;-AUC_comp$lassosum_AUC-AUC_comp$DBSLMM_AUC
  
pheno&lt;-c(&#39;Intelligence&#39;,&#39;BMI&#39;,&#39;Height&#39;)

R2_comp&lt;-NULL
for(i in 1:length(pheno)){
  lassosum_R2_i&lt;-NULL
  for(k in 1:10){
    lassosum_R2_i_k&lt;-read.table(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/10FC_GWAS/meta/lassosum/&#39;,pheno[i],&#39;/Subset_&#39;,k,&#39;/lassosum_pseudo_&#39;,pheno[i],&#39;_Subset_&#39;,k,&#39;.log&#39;), sep=&#39;&amp;&#39;, header=F)$V1
    
    lasso_R2_i_k_r&lt;-as.numeric(gsub(&#39;value = &#39;,&#39;&#39;,lassosum_R2_i_k[grepl(&#39;value = &#39;, lassosum_R2_i_k)]))
    lasso_R2_i_k_R2&lt;-lasso_R2_i_k_r^2

    lassosum_R2_i&lt;-rbind(lassosum_R2_i, data.frame( Subset=k,
                                                      R2=lasso_R2_i_k_R2))
  }
  
  lassosum_R2_i_average&lt;-mean(lassosum_R2_i$R2)
  
  pheno_res_pTclump&lt;-read.csv(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/pTclump/&#39;,pheno[i],&#39;_PRS_assoc_res.csv&#39;))
  pheno_res_pTclump$pT&lt;-as.numeric(gsub(&#39;_&#39;,&#39;-&#39;,pheno_res_pTclump$pT))
  Obs_R2_pTclump&lt;-pheno_res_pTclump$R2_obs[pheno_res_pTclump$pT == 1]
  
  pheno_res_DBSLMM&lt;-read.csv(paste0(&#39;/users/k1806347/brc_scratch/Data/UKBB/ValidationArena/PolygenicScores/DBSLMM/Pheno_PRS_assoc_res.csv&#39;))
  pheno_res_DBSLMM&lt;-pheno_res_DBSLMM[pheno_res_DBSLMM$Phenotype == pheno[i],]
  Obs_R2_DBSLMM&lt;-pheno_res_DBSLMM$R2_obs
  
  R2_comp&lt;-rbind(R2_comp, data.frame(Phenotype=pheno[i],
                                       pTclump_R2=Obs_R2_pTclump,
                                       DBSLMM_R2=Obs_R2_DBSLMM,
                                       lassosum_R2=lassosum_R2_i_average))
}

# It does quite well for binary outcome, but then </code></pre>
</details>
<p><br/></p>
<hr />
</div>
</div>
</div>
</div>
<div id="prepare-input-for-impute.me-module" class="section level1">
<h1><span class="header-section-number">3</span> Prepare input for Impute.Me module</h1>
<p>I need to download GWAS sumstats in an automated way, estimate the underlying heritability and produce reference standardised score files. GWAS sumstats will be downloaded from the GWAS catalogue.</p>
<div id="download-gwas-sumstats" class="section level2">
<h2><span class="header-section-number">3.1</span> Download GWAS sumstats</h2>
<details>
<p><summary>Show results</summary></p>
<pre class="r"><code># Download latest GWAS catalogue index (22/11/2020)
# https://www.ebi.ac.uk/gwas/downloads/summary-statistics

# Read in the csv
gwas_list&lt;-read.csv(&#39;~/brc_scratch/Data/GWAS_sumstats/GWAS_catalog/list_gwas_summary_statistics_22_Nov_2020.csv&#39;)

# Create a list GWAS catalogue IDs
gwas_ids&lt;-c(&#39;GCST006901&#39;,&#39;GCST006900&#39;,&#39;GCST003045&#39;,&#39;GCST004773&#39;,&#39;GCST004296&#39;,&#39;GCST006572&#39;)
gwas_list_subset&lt;-gwas_list[(gwas_list$Study.accession %in% gwas_ids),]

# List ftp downloads page
system(paste0(&#39;wget ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/  -P ~/brc_scratch/Data/GWAS_sumstats/GWAS_catalog/&#39;))
ftp_list&lt;-read.table(&#39;~/brc_scratch/Data/GWAS_sumstats/GWAS_catalog/index.html&#39;, sep=&#39;&amp;&#39;, header=F)$V1
ftp_list&lt;-ftp_list[grepl(paste(gwas_list_subset$Study.accession, collapse=&#39;|&#39;), ftp_list)]
ftp_list&lt;-gsub(&#39;&gt;.*&#39;,&#39;&#39;,gsub(&#39;.*href=&#39;,&#39;&#39;,ftp_list))

# Download these summary statistics
for(gwas_i in ftp_list){
  system(paste0(&#39;wget &#39;,gwas_i,&#39; -O ~/brc_scratch/Data/GWAS_sumstats/GWAS_catalog/temp.txt&#39;))
  temp_list&lt;-read.table(&#39;~/brc_scratch/Data/GWAS_sumstats/GWAS_catalog/temp.txt&#39;, sep=&#39;&amp;&#39;, header=F)$V1
  temp_list&lt;-temp_list[grepl(&#39;href&#39;, temp_list)]
  temp_list&lt;-gsub(&#39;&gt;.*&#39;,&#39;&#39;,gsub(&#39;.*href=&#39;,&#39;&#39;,temp_list))
  if(sum(grepl(&#39;harmonised&#39;, temp_list) == T) &gt; 0){
      system(paste0(&#39;wget -r &#39;,gwas_i,&#39;harmonised/ -P ~/brc_scratch/Data/GWAS_sumstats/GWAS_catalog/&#39;))
  } else {
      system(paste0(&#39;wget -r &#39;,gwas_i,&#39; -P ~/brc_scratch/Data/GWAS_sumstats/GWAS_catalog/&#39;))
  }
}

# After looking through the results for these studies, they are often not full summary statistics. Some are not in harmonised format, and some contain trans ancestry data. I think using the KCL repository might be a better option.</code></pre>
</details>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
